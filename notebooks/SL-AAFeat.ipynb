{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06baf6b6-c122-470e-a9da-bd30d0942c52",
   "metadata": {
    "id": "06baf6b6-c122-470e-a9da-bd30d0942c52"
   },
   "source": [
    "**Project:** Protein Function Improvement through Amino-acid Feature-based Machine Learning Approach\n",
    "\n",
    "**Goal:**\n",
    "The goal of this project is to predict fluorescence intensity from amino acid sequences. Here, the protein sequence is represented using features extracted from the amino acid sequences, and the project involves training various models and selecting the most optimal one for further prediction. A similar workflow can be beneficial for predicting other protein function properties.\n",
    "\n",
    "**Steps:**\n",
    "1. Data Collection\n",
    "2. Feature Extraction\n",
    "3. Preprocessing and Normalization\n",
    "4. Model Selection and Training\n",
    "5. Model Evaluation and Hyperparameter Tuning\n",
    "6. Model Prediction\n",
    "\n",
    "**Step 1: Data Collection**\n",
    "<br>Dataset: Deep Mutational Scanning (DMS) data for the parent green fluorescent protein (GFP) protein and its mutants, originally derived from Sarkisyan et al. (2016). Pre-computed data splits from Rao et al. (2020) will be used. The pre-computed data splits contain 'train', 'valid', and 'test' sets. Evaluation will be made using the 'test' set across all approaches. For testing purposes, only 10% of the 'train' set will be retained.  \n",
    "\n",
    "`Protein sequence`: GFP variants<br>\n",
    "`Target value`: Fluorescence intensity<br>\n",
    "\n",
    "**File:** fluorescence.csv (use scripts/create_dataset.py to create dataset for use)\n",
    "<br>`wget http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/fluorescence.tar.gz`\n",
    "<br>`tar -xzvf fluorescence.tar.gz`\n",
    "<br>`python scripts/create_dataset.py -d fluorescence -o fluorescence.csv -t 0.1 (use -t 1.0 to use full dataset)`\n",
    "<br>`rm -rf fluorescence`\n",
    "<br>`rm fluorescence.tar.gz`\n",
    "\n",
    "**Prerequisites:**\n",
    "JupyterLab is an open-source web-based interactive development environment (IDE) primarily used for working with Jupyter notebooks, code, and data (advanced version of Jupyter Notebook). It is generally a good idea to install JupyterLab in a dedicated environment to avoid conflicts with other packages.\n",
    "\n",
    "To create a new conda environment, run:<br>\n",
    "\t&emsp;&emsp;&emsp;`conda create -n mlearn python=3.12`<br>\n",
    "\t&emsp;&emsp;&emsp;`conda activate mlearn`\n",
    "\n",
    "*(alternatively, create conda environment from the provided yml file: `conda env create -f mlearn.yml`)*<br>\n",
    "\n",
    "To install jupyer-lab, run:<br>\n",
    "    &emsp;&emsp;&emsp;`conda install -c conda-forge jupyterlab`\n",
    "\n",
    "Once JupyterLab is installed, you can start it by running:<br>\n",
    "    &emsp;&emsp;&emsp;`jupyter-lab`\n",
    "\n",
    "This will launch JupyterLab in your default web browser. It will typically open at `http://localhost:8888` (or another port if 8888 is already in use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ad5ef-ef25-4479-bb12-f3e407a3426e",
   "metadata": {
    "id": "a79ad5ef-ef25-4479-bb12-f3e407a3426e"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install pandas peptides scipy scikit-learn ipywidgets optuna-integration hyperopt scikit-optimize\n",
    "\n",
    "!pip install lazypredict\n",
    "#!pip install autogluon\n",
    "#!pip install mljar-supervised\n",
    "#!pip install pycaret\n",
    "# note: compatibility issue might arise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03909e4-ecad-40da-8b41-0da0b83b7338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn-intelex==2023.1.1 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn-intelex==2023.1.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# try to install scikit-learn-intelex for speed\n",
    "!pip install scikit-learn-intelex==2023.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3",
   "metadata": {
    "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3"
   },
   "outputs": [],
   "source": [
    "# When running in Google Colab\n",
    "# (i) Create a copy of data and notebook in your own drive\n",
    "# Mount Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88f714-48e7-49be-aefb-753677b26c6b",
   "metadata": {},
   "source": [
    "**Step 2: Data Preparation and Processing**\n",
    "<br>Use precomputed data splits (in this study) or perform data splits for training using train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "845d3f05-11fe-480c-a18d-799bb4292644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments\n",
    "seqcol = 'sequence' # column to sequence\n",
    "ycol = 'label' # column to score/label\n",
    "seed = 42 # random seed to use\n",
    "test_size = 0.2 # test size for data splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1095c8-6842-4259-8811-f78fc4e9bd9e",
   "metadata": {
    "id": "db1095c8-6842-4259-8811-f78fc4e9bd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence     label  split\n",
      "0  SKGEELFTGVVPILVELDGEVNGHKFSVSGEGEGDATYGKLTLKFI...  3.586350  train\n",
      "1  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...  3.612519  train\n",
      "5173 2097 524 2552\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (A) In the case where data split is pre-computed\n",
    "# Load training data\n",
    "# csv_file=\"/content/drive/MyDrive/Colab Notebooks/fluorescence.csv\"\n",
    "csv_file = \"../data/fluorescence.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.head(2))\n",
    "print(len(df), len(df[df['split']=='train']), len(df[df['split']=='valid']), len(df[df['split']=='test']))\n",
    "\n",
    "# (B) In the case where the data split is not defined\n",
    "def get_data_split(df, test_size):\n",
    "    train, test = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    train['split'] = 'train'\n",
    "    test['split'] = 'test'\n",
    "    df = pd.concat([train,test])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb37803-eb11-4b39-84a7-dc6a5f40a421",
   "metadata": {
    "id": "bdb37803-eb11-4b39-84a7-dc6a5f40a421"
   },
   "source": [
    "**Step 3: Feature Extraction from Amino Acid Sequences**\n",
    "<br>Each amino acid can be represented by a vector of physicochemical properties like hydrophobicity, charge, size, etc. A total of 8 different protein descriptor sets will be used in this study: Z-scales, VHSE, T-scales, ST-scales, MS-WHIM, FASGAI, BLOSUM and ProtFP (https://pmc.ncbi.nlm.nih.gov/articles/PMC3848949/). Here, `peptides` Python package is used to compute descriptors for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
   "metadata": {
    "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
    "outputId": "6f97fc86-d9f6-4680-9251-2acfd234cbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence    0     1    2     3  \\\n",
      "0  SKGEELFTGVVPILVELDGEVNGHKFSVSGEGEGDATYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "1  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "\n",
      "      4    5     6     7     8  ...   703   704   705   706   707   708   709  \\\n",
      "0  0.08  0.6 -0.31 -0.28 -0.75  ...  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08   \n",
      "1  0.08  0.6 -0.31 -0.28 -0.75  ...  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08   \n",
      "\n",
      "   710     label  split  \n",
      "0  0.6  3.586350  train  \n",
      "1  0.6  3.612519  train  \n",
      "\n",
      "[2 rows x 714 columns]\n"
     ]
    }
   ],
   "source": [
    "import peptides\n",
    "\n",
    "# Define a function to compute features, adding new columns\n",
    "# Available: blosum_indices fasgai_vectors ms_whim_scores protfp_descriptors st_scales t_scales vhse_scale z_scales\n",
    "def compute_features(df, seqcol, feature):\n",
    "    # Available features: available: blosum_indices fasgai_vectors ms_whim_scores protfp_descriptors st_scales t_scales vhse_scale z_scales\n",
    "    # and many more (refer to peptides package in github: https://github.com/althonos/peptides.py)\n",
    "\n",
    "    # Get features from a descriptor\n",
    "    data1=[[list(getattr(peptides.Peptide(a_a), feature)()) for a_a in list(seq)] for seq in df[seqcol]]\n",
    "    data2=[[s for j in k for s in j] for k in data1]\n",
    "\n",
    "    # Retrieve features (X) and target values (y)\n",
    "    X = pd.DataFrame(data2)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Get features for all sequences in the dataset (X)\n",
    "X = compute_features(df, seqcol, 'ms_whim_scores')\n",
    "\n",
    "# Combine the columns\n",
    "cols = [n for n in df.columns if n != seqcol]\n",
    "features = pd.concat([df[seqcol], X, df[cols]],axis=1)\n",
    "print(features.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c661e-7111-4c12-a172-135c91ff7dec",
   "metadata": {
    "id": "8d0c661e-7111-4c12-a172-135c91ff7dec"
   },
   "source": [
    "**Step 3: Data Preprocessing and Normalization**\n",
    "<br>Once the features are extracted, the dataset is divided into training and test sets to ensure that the model generalizes well to unseen data. For small datasets, k-fold cross-validation can be used to get more reliable performance estimates (used in this study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b22b4cd7-6799-41f8-8478-c68ff8b4622e",
   "metadata": {
    "id": "b22b4cd7-6799-41f8-8478-c68ff8b4622e"
   },
   "outputs": [],
   "source": [
    "# Define train set for training\n",
    "train = features[features['split']=='train']\n",
    "X_train = train.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "y_train = train[ycol]\n",
    "\n",
    "# Define test set for evaluation\n",
    "test = features[features['split']=='test']\n",
    "X_test = test.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "y_test = test[ycol]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018442f-8fc3-4ae8-8868-95d465dcbb81",
   "metadata": {},
   "source": [
    "Define scoring metric to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b5d7ca-f5e9-481d-a054-67e40037418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this study, we will use spearman's correlation as the metric for evaluation\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def spearmanr_metric(y_true, y_pred):\n",
    "    r = spearmanr(a=y_true, b=y_pred)\n",
    "    return r[0]\n",
    "    \n",
    "spearmanr_metric.__name__=\"Spearman\"\n",
    "\n",
    "def pearsonr_metric(y_true, y_pred):\n",
    "    r = pearsonr(y_true, y_pred)\n",
    "    return r[0] \n",
    "\n",
    "def set_scoring(metric):\n",
    "    if metric == 'mse':\n",
    "        return 'neg_root_mean_squared_error'\n",
    "    if metric == 'acc':\n",
    "        return 'accuracy'\n",
    "    elif metric == 'spr':\n",
    "        return make_scorer(spearmanr_metric)\n",
    "    elif metric == 'per':\n",
    "        return make_scorer(pearsonr_metric)\n",
    "    else:\n",
    "        print('wrong metric', metric)\n",
    "        exit()\n",
    "def evaluate_test(y_test, y_pred, metric):\n",
    "    if metric == 'mse':\n",
    "        return root_mean_squared_error(y_test, y_pred)\n",
    "    if metric == 'acc':\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    elif metric == 'spr':\n",
    "        return spearmanr(y_test, y_pred)[0]\n",
    "    elif metric == 'per':\n",
    "        return pearsonr(y_test, y_pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b",
   "metadata": {
    "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b"
   },
   "source": [
    "**Step 4: Model Selection: Use lazypredict to quickly compare different models on a regression task**\n",
    "<br> The first step in supervised learning is to compare and select a model for the dataset. \n",
    "\n",
    "In this step, the `LazyPredict` Python package is used to identify best model architecture that is suitable for the features. This Python package is designed to quickly try multiple machine learning models with minimal code (using default hyperparameters), making it easier for users to compare the performance of various algorithms without needing to manually implement each model. The package use its own data normalization steps, thus data pre-processing is not needed in this case.\n",
    "\n",
    "<br>Why lazypredict?\n",
    "* **Quick Prototyping:** To quickly get a sense of which machine learning models are working best for the dataset before diving into hyperparameter tuning and feature engineering.\n",
    "* **Model Comparison:** To get potential model candidates that might work best for a given problem/dataset, without having to implement them manually.\n",
    "* **Exploratory Data Analysis:** To quickly assess the performance of different models to gain insights into how well they fit the data.\n",
    "\n",
    "Several other AutoML tools are available: AutoGluon, PyCaret, and MLJAR (Note: compatibility issues might arise (e.g. incompatible version of scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a6745241-5930-42e4-a98f-ed77cc5f3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
   "metadata": {
    "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
    "outputId": "fe8d1b2c-74b8-4ed7-b62e-5fbb89c80923"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fece0fecdb064f6da5e6fb9d68b4967f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostRegressor', 'R-Squared': -1.156185519657695, 'Adjusted R-Squared': -1.9893637286123802, 'RMSE': 1.4465167991129086, 'Time taken': 0.08756494522094727, 'Spearman': 0.12303343848599761}\n",
      "{'Model': 'BaggingRegressor', 'R-Squared': 0.33790005657390076, 'Adjusted R-Squared': 0.08205600234783739, 'RMSE': 0.8015713655042572, 'Time taken': 1.7501928806304932, 'Spearman': 0.6384760346029084}\n",
      "{'Model': 'BayesianRidge', 'R-Squared': -0.39027238640159645, 'Adjusted R-Squared': -0.927491770494822, 'RMSE': 1.1615298297320122, 'Time taken': 0.9538218975067139, 'Spearman': 0.6029455107878958}\n",
      "{'Model': 'DecisionTreeRegressor', 'R-Squared': 0.21753768400411888, 'Adjusted R-Squared': -0.08481596092689814, 'RMSE': 0.8713891228017425, 'Time taken': 0.4300577640533447, 'Spearman': 0.5957814267169287}\n",
      "{'Model': 'DummyRegressor', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -2.027733543022581, 'RMSE': 1.4557705433826131, 'Time taken': 0.0262148380279541, 'Spearman': nan}\n",
      "{'Model': 'ElasticNet', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -2.027733543022581, 'RMSE': 1.4557705433826131, 'Time taken': 0.08899307250976562, 'Spearman': nan}\n",
      "{'Model': 'ElasticNetCV', 'R-Squared': -0.29979947394134543, 'Adjusted R-Squared': -0.8020589445784632, 'RMSE': 1.1231004404502893, 'Time taken': 27.43849778175354, 'Spearman': 0.6016331703013691}\n",
      "{'Model': 'ExtraTreeRegressor', 'R-Squared': 0.08316019581814671, 'Adjusted R-Squared': -0.271118663297776, 'RMSE': 0.9432507278618475, 'Time taken': 0.32521963119506836, 'Spearman': 0.5668640761110992}\n",
      "{'Model': 'ExtraTreesRegressor', 'R-Squared': 0.3068134525975029, 'Adjusted R-Squared': 0.038957129117516254, 'RMSE': 0.8201730260209893, 'Time taken': 22.878672122955322, 'Spearman': 0.6318177131667282}\n",
      "{'Model': 'GammaRegressor', 'R-Squared': -0.09997812883897539, 'Adjusted R-Squared': -0.5250240253631664, 'RMSE': 1.0331715630179545, 'Time taken': 0.07247686386108398, 'Spearman': 0.5874813565529354}\n",
      "{'Model': 'GaussianProcessRegressor', 'R-Squared': -4.606180718585686, 'Adjusted R-Squared': -6.772482072343525, 'RMSE': 2.332458717385729, 'Time taken': 3.9221928119659424, 'Spearman': 0.4717243177873512}\n",
      "{'Model': 'GradientBoostingRegressor', 'R-Squared': -0.20670362220718985, 'Adjusted R-Squared': -0.6729896414405117, 'RMSE': 1.0821332146028888, 'Time taken': 0.7109718322753906, 'Spearman': 0.5643733080788851}\n",
      "{'Model': 'HistGradientBoostingRegressor', 'R-Squared': -1.413806053291387, 'Adjusted R-Squared': -2.346532196709961, 'RMSE': 1.530493907659855, 'Time taken': 1.0754539966583252, 'Spearman': 0.36239578068682576}\n",
      "{'Model': 'HuberRegressor', 'R-Squared': -1.4432204061179448, 'Adjusted R-Squared': -2.3873126391341724, 'RMSE': 1.5397908786358472, 'Time taken': 0.67476487159729, 'Spearman': 0.593357388705818}\n",
      "{'Model': 'KNeighborsRegressor', 'R-Squared': -0.8407369726585998, 'Adjusted R-Squared': -1.5520217485065695, 'RMSE': 1.3365225385394943, 'Time taken': 0.15639281272888184, 'Spearman': 0.5008276507880404}\n",
      "{'Model': 'KernelRidge', 'R-Squared': -11.300058657634718, 'Adjusted R-Squared': -16.052961758492483, 'RMSE': 3.454888297176057, 'Time taken': 0.6378908157348633, 'Spearman': 0.5889612521016651}\n",
      "{'Model': 'Lars', 'R-Squared': -5.855678321102454e+38, 'Adjusted R-Squared': -8.118388802789327e+38, 'RMSE': 2.3837956591962305e+19, 'Time taken': 0.2528529167175293, 'Spearman': 0.04487491101789577}\n",
      "{'Model': 'LarsCV', 'R-Squared': -0.37872764949514215, 'Adjusted R-Squared': -0.9114859966641891, 'RMSE': 1.1566971398776142, 'Time taken': 1.0588932037353516, 'Spearman': 0.5377351072120164}\n",
      "{'Model': 'Lasso', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -2.027733543022581, 'RMSE': 1.4557705433826131, 'Time taken': 0.1220090389251709, 'Spearman': nan}\n",
      "{'Model': 'LassoCV', 'R-Squared': -0.3161631751478362, 'Adjusted R-Squared': -0.824745793370723, 'RMSE': 1.1301479117677244, 'Time taken': 32.832457065582275, 'Spearman': 0.6012624882309353}\n",
      "{'Model': 'LassoLars', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -2.027733543022581, 'RMSE': 1.4557705433826131, 'Time taken': 0.20541000366210938, 'Spearman': nan}\n",
      "{'Model': 'LassoLarsCV', 'R-Squared': -0.3413863956201504, 'Adjusted R-Squared': -0.8597155952320672, 'RMSE': 1.140925712770823, 'Time taken': 0.5361499786376953, 'Spearman': 0.5482173272301848}\n",
      "{'Model': 'LassoLarsIC', 'R-Squared': -0.3413863956201504, 'Adjusted R-Squared': -0.8597155952320672, 'RMSE': 1.140925712770823, 'Time taken': 0.5537679195404053, 'Spearman': 0.5482173272301848}\n",
      "{'Model': 'LinearRegression', 'R-Squared': -4.420402786570471e+28, 'Adjusted R-Squared': -6.128504080728951e+28, 'RMSE': 207114898881179.47, 'Time taken': 0.45020008087158203, 'Spearman': 0.5267314768900561}\n",
      "{'Model': 'LinearSVR', 'R-Squared': -1.851398015723221, 'Adjusted R-Squared': -2.953215401146705, 'RMSE': 1.6634484268554806, 'Time taken': 2.11077880859375, 'Spearman': 0.568090204784852}\n",
      "{'Model': 'MLPRegressor', 'R-Squared': -280.5009917198152, 'Adjusted R-Squared': -389.2766466724177, 'RMSE': 16.528003027665726, 'Time taken': 1.0668821334838867, 'Spearman': 0.4525825270056762}\n",
      "{'Model': 'NuSVR', 'R-Squared': -0.030215309872968055, 'Adjusted R-Squared': -0.4283039431988813, 'RMSE': 0.9998720280257722, 'Time taken': 2.776176929473877, 'Spearman': 0.6286971769740632}\n",
      "{'Model': 'OrthogonalMatchingPursuit', 'R-Squared': -0.8252051874789164, 'Adjusted R-Squared': -1.5304882789449543, 'RMSE': 1.3308719333150125, 'Time taken': 0.04411911964416504, 'Spearman': 0.5299679088617351}\n",
      "{'Model': 'OrthogonalMatchingPursuitCV', 'R-Squared': -0.6786263151967626, 'Adjusted R-Squared': -1.3272694185146423, 'RMSE': 1.2763137049625952, 'Time taken': 0.1557600498199463, 'Spearman': 0.5337375647360488}\n",
      "{'Model': 'PassiveAggressiveRegressor', 'R-Squared': -1.1354554897372342, 'Adjusted R-Squared': -1.9606233447389592, 'RMSE': 1.4395464436908896, 'Time taken': 0.11322784423828125, 'Spearman': 0.5536713652918923}\n",
      "{'Model': 'PoissonRegressor', 'R-Squared': -0.17953905102378265, 'Adjusted R-Squared': -0.6353283256313422, 'RMSE': 1.0698837246312414, 'Time taken': 0.06519699096679688, 'Spearman': 0.6015028922669726}\n",
      "{'Model': 'QuantileRegressor', 'R-Squared': -2.160015827413052, 'Adjusted R-Squared': -3.3810871607232045, 'RMSE': 1.7511568530421915, 'Time taken': 0.7831239700317383, 'Spearman': nan}\n",
      "{'Model': 'RANSACRegressor', 'R-Squared': -1.6899712927227107e+28, 'Adjusted R-Squared': -2.342998243334584e+28, 'RMSE': 128061941826707.28, 'Time taken': 39.04080104827881, 'Spearman': 0.02447850374867677}\n",
      "{'Model': 'RandomForestRegressor', 'R-Squared': 0.3856705190805245, 'Adjusted R-Squared': 0.1482855946600098, 'RMSE': 0.7721134142016938, 'Time taken': 17.43430995941162, 'Spearman': 0.6501500844121038}\n",
      "{'Model': 'Ridge', 'R-Squared': -1.2216728460137305, 'Adjusted R-Squared': -2.0801562120549058, 'RMSE': 1.4683191826957231, 'Time taken': 0.3852667808532715, 'Spearman': 0.5889612521016651}\n",
      "{'Model': 'RidgeCV', 'R-Squared': -0.8663887303990667, 'Adjusted R-Squared': -1.5875856800260975, 'RMSE': 1.3458029342372044, 'Time taken': 0.6454401016235352, 'Spearman': 0.595208732700848}\n",
      "{'Model': 'SGDRegressor', 'R-Squared': -442786092002.3655, 'Adjusted R-Squared': -613884413423.2312, 'RMSE': 655507.107059673, 'Time taken': 0.07262372970581055, 'Spearman': 0.04897561804192504}\n",
      "{'Model': 'SVR', 'R-Squared': 0.03326980372173316, 'Adjusted R-Squared': -0.34028735364448837, 'RMSE': 0.968574567340912, 'Time taken': 2.2309231758117676, 'Spearman': 0.6272681926523713}\n",
      "{'Model': 'TransformedTargetRegressor', 'R-Squared': -4.420402786570471e+28, 'Adjusted R-Squared': -6.128504080728951e+28, 'RMSE': 207114898881179.47, 'Time taken': 0.17610430717468262, 'Spearman': 0.5267314768900561}\n",
      "{'Model': 'TweedieRegressor', 'R-Squared': -0.14729426610081964, 'Adjusted R-Squared': -0.590623735229995, 'RMSE': 1.0551588130773157, 'Time taken': 0.2919197082519531, 'Spearman': 0.5900743059376362}\n",
      "{'Model': 'XGBRegressor', 'R-Squared': 0.020532829177347778, 'Adjusted R-Squared': -0.3579460612872749, 'RMSE': 0.9749343252927343, 'Time taken': 0.3225579261779785, 'Spearman': 0.6583806823777346}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.009788 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1676\n",
      "[LightGBM] [Info] Number of data points in the train set: 2097, number of used features: 309\n",
      "[LightGBM] [Info] Start training from score 3.186066\n",
      "{'Model': 'LGBMRegressor', 'R-Squared': -1.4406742450994585, 'Adjusted R-Squared': -2.3837826082873472, 'RMSE': 1.538988335920135, 'Time taken': 0.4273190498352051, 'Spearman': 0.35329788479856367}\n",
      "                               Spearman  Time Taken\n",
      "Model                                              \n",
      "XGBRegressor                       0.66        0.32\n",
      "RandomForestRegressor              0.65       17.43\n",
      "BaggingRegressor                   0.64        1.75\n",
      "ExtraTreesRegressor                0.63       22.88\n",
      "NuSVR                              0.63        2.78\n",
      "SVR                                0.63        2.23\n",
      "BayesianRidge                      0.60        0.95\n",
      "ElasticNetCV                       0.60       27.44\n",
      "PoissonRegressor                   0.60        0.06\n",
      "LassoCV                            0.60       32.83\n",
      "DecisionTreeRegressor              0.60        0.43\n",
      "RidgeCV                            0.60        0.62\n",
      "HuberRegressor                     0.59        0.67\n",
      "TweedieRegressor                   0.59        0.29\n",
      "Ridge                              0.59        0.38\n",
      "KernelRidge                        0.59        0.64\n",
      "GammaRegressor                     0.59        0.07\n",
      "LinearSVR                          0.57        2.11\n",
      "ExtraTreeRegressor                 0.57        0.32\n",
      "GradientBoostingRegressor          0.56        0.71\n",
      "PassiveAggressiveRegressor         0.55        0.11\n",
      "LassoLarsCV                        0.55        0.54\n",
      "LassoLarsIC                        0.55        0.55\n",
      "LarsCV                             0.54        1.06\n",
      "OrthogonalMatchingPursuitCV        0.53        0.16\n",
      "OrthogonalMatchingPursuit          0.53        0.04\n",
      "TransformedTargetRegressor         0.53        0.17\n",
      "LinearRegression                   0.53        0.45\n",
      "KNeighborsRegressor                0.50        0.16\n",
      "GaussianProcessRegressor           0.47        3.92\n",
      "MLPRegressor                       0.45        1.07\n",
      "HistGradientBoostingRegressor      0.36        1.07\n",
      "LGBMRegressor                      0.35        0.43\n",
      "AdaBoostRegressor                  0.12        0.09\n",
      "SGDRegressor                       0.05        0.07\n",
      "Lars                               0.04        0.25\n",
      "RANSACRegressor                    0.02       39.04\n",
      "DummyRegressor                      NaN        0.03\n",
      "LassoLars                           NaN        0.21\n",
      "Lasso                               NaN        0.12\n",
      "ElasticNet                          NaN        0.09\n",
      "QuantileRegressor                   NaN        0.78\n"
     ]
    }
   ],
   "source": [
    "# Incompatibilities between lazypredict and sklearn might cause errors\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import make_scorer\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "# Create a LazyRegressor object\n",
    "rgs = LazyRegressor(verbose=1, ignore_warnings=False, custom_metric=spearmanr_metric)\n",
    "\n",
    "# Fit the models and get a comparison\n",
    "models, predictions = rgs.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Sort results and show only the selected metric\n",
    "sorted_models = models.sort_values(by='Spearman', ascending=False)\n",
    "\n",
    "# Show model comparison results\n",
    "print(sorted_models[['Spearman','Time Taken']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecd25c35-0808-431f-b8c3-f933e4edeb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Adjusted R-Squared  R-Squared  RMSE  Time Taken  \\\n",
      "Model                                                                    \n",
      "XGBRegressor                        -0.36       0.02  0.97        0.32   \n",
      "RandomForestRegressor                0.15       0.39  0.77       17.43   \n",
      "BaggingRegressor                     0.08       0.34  0.80        1.75   \n",
      "ExtraTreesRegressor                  0.04       0.31  0.82       22.88   \n",
      "NuSVR                               -0.43      -0.03  1.00        2.78   \n",
      "\n",
      "                       Spearman  \n",
      "Model                            \n",
      "XGBRegressor               0.66  \n",
      "RandomForestRegressor      0.65  \n",
      "BaggingRegressor           0.64  \n",
      "ExtraTreesRegressor        0.63  \n",
      "NuSVR                      0.63  \n"
     ]
    }
   ],
   "source": [
    "# Print top 5 models for evaluation\n",
    "print(sorted_models.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85958dc2-06cd-48a3-a688-af839b194209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try autogluon (much better compatibility with latest sklearn) [NOT TESTED DUE TO COMPATIBILITY ISSUE] \n",
    "# It might take some time, but it uses lesser models than lazypredict and pycaret\n",
    "from autogluon.tabular import TabularPredictor\n",
    "train = pd.concat([X_train,y_train], axis=1)\n",
    "test = pd.concat([X_test,y_test], axis=1)\n",
    "\n",
    "predictor = TabularPredictor(label=ycol, verbosity=1, eval_metric=\"spearmanr\", problem_type=\"regression\").fit(train)\n",
    "y_pred = predictor.predict(test.drop(columns=[ycol]))\n",
    "predictor.evaluate(test)\n",
    "predictor.leaderboard(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4044b-2337-4c99-abef-533761fed16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try pycaret  [NOT TESTED DUE TO COMPATIBILITY ISSUE] \n",
    "from pycaret.regression import *\n",
    "s = setup(train, target = ycol, session_id = 123, verbose=1)\n",
    "\n",
    "# model training and selection\n",
    "best = compare_models()\n",
    "\n",
    "# evaluate trained model\n",
    "evaluate_model(best)\n",
    "\n",
    "# predict on hold-out/test set\n",
    "predict_model(best, data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08413e4-f251-400f-91c9-90dc43af76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try MLJAR  [NOT TESTED DUE TO COMPATIBILITY ISSUE] \n",
    "from supervised.automl import AutoML\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dd001-aa62-491a-899f-07d0b1bb2b23",
   "metadata": {
    "id": "615dd001-aa62-491a-899f-07d0b1bb2b23"
   },
   "source": [
    "**Step 5: Model Construction and Hyperparameter Tuning: Use Nested K-fold Cross Validation to Estimate Model Performance**\n",
    "<br>Data scaling and normalization are necessary to ensure that all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f60147d-5701-4d5e-b495-cc5278227e24",
   "metadata": {
    "id": "2f60147d-5701-4d5e-b495-cc5278227e24"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the train set \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set (not fit to ensure the set remains unseen)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068d0ac-2afe-487f-b157-a34ac40595d9",
   "metadata": {
    "id": "3068d0ac-2afe-487f-b157-a34ac40595d9"
   },
   "source": [
    "Hyperparameter optimization (HPO) is the process of finding the best set of hyperparameters for a machine learning model to maximize its performance on a validation (test) set.\n",
    "\n",
    "**(A) Nested cross-validation for small dataset**\n",
    "When training data are limited, double cross-validation (nested cross-validation) is the standard approach for evaluating and comparing machine learning models with tuned hyperparameters. Nested cross-validation is particularly important when a fixed, untouched test set is not available. Without nested cross-validation, the same data may be used both for hyperparameter selection and performance evaluation, leading to optimistically biased estimates and increased risk of overfitting.\n",
    "\n",
    "* **Inner Cross-Validation (Hyperparameter Tuning):**\n",
    "  - Split the data into N_SPLITS folds\n",
    "  - Evaluate the model's performance using each fold as a validation set while training on the remaining folds\n",
    "  - Estimate the best set of hyperparameters for the model\n",
    "* **Outer Cross-Validation (Model Evaluation):**\n",
    "  - Assess the model's generalization performance\n",
    "  - Ensures that the model is tested on different, unseen data splits\n",
    "\n",
    "**(B) Standard Inner Cross-validation**\n",
    "<br> K-fold cross-validation can be used to estimate model performance by producing multiple performance estimates across different data splits, which is particularly suitable for medium to large datasets.\n",
    "\n",
    "*Hyperparameter tuning*\n",
    "<br>Here, Support Vector Regressor (SVR) will be used for further training and evaluation. To find the optimal hyperparameters for the model, several HPO algorithms will be tested.\n",
    "\n",
    "*Why use K-fold instead of a single train-test split?*\n",
    "<br>A single train-test split can produce a noisy or biased estimate of model performance, particularly if the test set is small or not representative of the population. K-fold cross-validation mitigates this issue by evaluating the model across multiple folds of the data, providing a more robust and stable estimate of generalization performance.\n",
    "\n",
    "*Example on how to use K-fold with GridSearchCV*\n",
    "```\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "gsmodel = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring=scoring, verbose=1)\n",
    "gsmodel.fit(X, y)\n",
    "print(gsmodel.best_params_)\n",
    "print(gsmodel.best_score_)\n",
    "```\n",
    "\n",
    "To allow proper comparison on different approaches that use train/test datasets (such as protein language model finetuning later on), hyperparameter tuning and model training are performed on the train set and evaluated on the test set. In the original study, several random seeds were used to obtain average performance scores of the model.\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "<br> Hyperparameter optimization can be performed using a range of search strategies that differ in efficiency and scalability. GridSearchCV performs an exhaustive evaluation of all parameter combinations in a predefined grid and is straightforward to implement but becomes computationally prohibitive as the search space grows. RandomizedSearchCV improves efficiency by sampling hyperparameter configurations at random, often achieving comparable performance with substantially fewer evaluations. More advanced methods, such as Optuna (including OptunaSearchCV), Hyperopt, and scikit-optimize (skopt), employ Bayesian or adaptive optimization strategies that leverage information from previous trials to guide subsequent searches toward promising regions of the parameter space. These methods are generally more sample-efficient and better suited for large or continuous search spaces, though they introduce additional complexity and dependencies. Overall, grid and random search provide simple and reproducible baselines, while Bayesian optimization frameworks offer improved efficiency for computationally expensive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ae447d8-1461-4dad-ba3d-da7de3f66cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# Define functions for HPO algorithms\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "import optuna\n",
    "from optuna.distributions import FloatDistribution, IntDistribution, CategoricalDistribution\n",
    "from optuna_integration.sklearn import OptunaSearchCV\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK \n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "import numpy as np\n",
    "\n",
    "# set verbose=1 to show errors, warning and std output (e.g. trial log)\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def default(model, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    val = evaluate_test(y_test, y_pred, metric)\n",
    "    return val, model\n",
    "    \n",
    "def train(model, params, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    val = evaluate_test(y_test, y_pred, metric)\n",
    "    return val, model\n",
    "     \n",
    "def grid_search(model, param_grid, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    reg = model\n",
    "    optim = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                         cv=5, scoring=scoring, verbose=0)\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def random_search(model, param_grid, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    reg = model\n",
    "    optim = RandomizedSearchCV(estimator=reg, param_distributions=param_grid, \n",
    "                               cv=5, scoring=scoring, verbose=0)\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def optuna_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = CategoricalDistribution(values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                param_distributions[name] = IntDistribution(low=values[0], high=values[1], log=True)\n",
    "            else:\n",
    "                param_distributions[name] = FloatDistribution(low=values[0],high=values[1],log=True)\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = CategoricalDistribution(values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "            \n",
    "    # set up training \n",
    "    reg = model\n",
    "    optim = optuna.integration.OptunaSearchCV(\n",
    "        reg, param_distributions, n_trials=n_trials, random_state=seed, scoring=scoring, verbose=0\n",
    "    )\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def hyperopt_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = hp.choice(name, values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                #param_distributions[name] = hp.quniform(name, values[0], values[1], q=1)\n",
    "                param_distributions[name] = hp.choice(name, np.arange(values[0], values[1], dtype=int))\n",
    "            else:\n",
    "                param_distributions[name] = hp.loguniform(\n",
    "                    name,\n",
    "                    np.log(values[0]),\n",
    "                    np.log(values[1])\n",
    "                )\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = hp.choice(name, values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "    # set up training\n",
    "    reg = model\n",
    "    def objective(params):\n",
    "        # Create model with sampled params\n",
    "        reg.set_params(**params)\n",
    "        # Cross-validation score (Hyperopt minimizes, so we negate scoring)\n",
    "        cv_scores = cross_val_score( model, X_train, y_train, scoring=scoring, cv=5, verbose=0)\n",
    "        loss = -np.mean(cv_scores)\n",
    "        return { 'loss': loss, 'status': STATUS_OK, 'params': params }\n",
    "    trials = Trials()\n",
    "    # Run TPE optimization\n",
    "    optim = fmin(fn=objective, space=param_distributions, algo=tpe.suggest, \n",
    "                 max_evals=n_trials, rstate=np.random.default_rng(seed), trials=trials, verbose=0 )\n",
    "    score, est = train(model, optim, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim)\n",
    "\n",
    "def skopt_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = Categorical(values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                param_distributions[name] = Integer(\n",
    "                    values[0],\n",
    "                    values[1]\n",
    "                )\n",
    "            else:\n",
    "                param_distributions[name] = Real(\n",
    "                    values[0],\n",
    "                    values[1],\n",
    "                    prior='log-uniform'\n",
    "                )\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = Categorical(values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "     # set up training\n",
    "    reg = model\n",
    "    optim = BayesSearchCV(\n",
    "        estimator=reg,\n",
    "        search_spaces=param_distributions,\n",
    "        n_iter=n_trials,\n",
    "        random_state=seed,\n",
    "        scoring=scoring,\n",
    "        cv=5,                 # same as OptunaSearchCV default\n",
    "        verbose=0,\n",
    "        optimizer_kwargs={\"base_estimator\": \"GP\"}  # Gaussian Process BO\n",
    "    )\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e34bc37-7a46-4074-b8ee-3784ec6e4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: 0.6273\n",
      "gridsearchCV: 0.6389\n",
      "randomsearchCV: 0.6389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-25 13:51:26,585] Trial 3 failed with parameters: {'gamma': 0.0003139312453909013, 'C': 0.16358767917148717, 'epsilon': 1.9886440867814372} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-12-25 13:51:26,585] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optuna: 0.6263\n",
      "hyperopt: 0.6441\n",
      "skopt: 0.5928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "# Define model\n",
    "model = SVR()\n",
    "metric = 'spr'\n",
    "n_trials=5\n",
    "param_grid = { 'gamma': [1e-5, 0.5], 'C':[0.01, 30.0], 'epsilon':[0.001, 2.0] }\n",
    "\n",
    "default, d_estimator = default(model, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'default: {default:.4f}')\n",
    "\n",
    "gridsearch, g_estimator, g_params = grid_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'gridsearchCV: {gridsearch:.4f}')\n",
    "\n",
    "randomsearch, r_estimator, r_params = random_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'randomsearchCV: {randomsearch:.4f}')\n",
    "\n",
    "optuna, o_estimator, o_params = optuna_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'optuna: {optuna:.4f}')\n",
    "\n",
    "hyperopt, h_estimator, h_params = hyperopt_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'hyperopt: {hyperopt:.4f}')\n",
    "\n",
    "skopt, s_estimator, s_params = skopt_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'skopt: {skopt:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c0fa390d-e90e-4b90-bbbd-6d87a5dc9e83",
   "metadata": {
    "id": "c0fa390d-e90e-4b90-bbbd-6d87a5dc9e83",
    "outputId": "3a0be294-c5f9-42da-dda6-e40e158091d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: 0.2035, {'C': 2, 'epsilon': 0.12207650529474924, 'gamma': 0.11072360241934348}\n",
      "Lasso: 0.6068, {'alpha': 0.001915263269470917}\n",
      "ETR: 0.6312, {'n_estimators': 25}\n"
     ]
    }
   ],
   "source": [
    "# Try three different model architecture\n",
    "# Note that the param_grid requires a range for float or integers (lower_value, upper_value)\n",
    "from sklearn.svm import SVR\n",
    "SVR_param_grid = { 'gamma': [0.1, 0.2], 'C':[20, 30], 'epsilon':[0.1, 1.5] }\n",
    "SVR_hyperopt, SVR_estimator, SVR_params = hyperopt_search(SVR(), SVR_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'SVR: {SVR_hyperopt:.4f}, {SVR_params}')\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso_param_grid = { 'alpha': [1e-3, 5e-3] }\n",
    "Lasso_hyperopt, Lasso_estimator, Lasso_params = hyperopt_search(Lasso(), Lasso_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'Lasso: {Lasso_hyperopt:.4f}, {Lasso_params}')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "ETR_param_grid = { 'n_estimators': [10, 50] }\n",
    "ETR_hyperopt, ETR_estimator, ETR_params = hyperopt_search(ExtraTreesRegressor(random_state=42), ETR_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'ETR: {ETR_hyperopt:.4f}, {ETR_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29042c9-688b-454e-8db9-885ee56f2a06",
   "metadata": {
    "id": "a29042c9-688b-454e-8db9-885ee56f2a06"
   },
   "source": [
    "Here, train set was used for hyperparameter tuning to obtain unbiased performance estimate using k-fold cross-validation (CV). Instead of using the best estimator output from HPO, the model is re-built with best parameters from HP tuning, re-trained with training set and evaluated on unseen test set. By using the train set for model construction and hyperparameter tuning, the test set remains truly unseen throughout the entire model selection process, ensuring an unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5",
   "metadata": {
    "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5"
   },
   "source": [
    "**Step 8: Model Prediction**\n",
    "<br>After evaluating several models based on their performance metrics, the best model can be saved for prediction on unseen or real data. For example, the model can be used to predict the fluorescent intensity of variants, given the amino acid features as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "c28777e3-fe60-4e65-893c-0f1b733f413d",
   "metadata": {
    "id": "c28777e3-fe60-4e65-893c-0f1b733f413d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                             mutant\n",
      "0  S1A  AKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "1  S1C  CKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "2  S1D  DKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "3  S1E  EKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "4  S1F  FKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "4503\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate all single-residue mutants\n",
    "def single_residue_mutants(sequence, amino_acids=None):\n",
    "    if amino_acids is None:\n",
    "        amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    mutants = []\n",
    "    for i, wt in enumerate(sequence):\n",
    "        for aa in amino_acids:\n",
    "            if aa != wt:\n",
    "                mutant = sequence[:i] + aa + sequence[i+1:]\n",
    "                label = f\"{wt}{i+1}{aa}\"\n",
    "                mutants.append([label]+[mutant])\n",
    "    mutants = pd.DataFrame(mutants)\n",
    "    mutants.columns = ['id','mutant']\n",
    "    return mutants\n",
    "\n",
    "# Generate the list of mutants for prediction\n",
    "sequence = \"SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "mutants = single_residue_mutants(sequence)\n",
    "print(mutants.head())\n",
    "print(len(mutants))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac",
   "metadata": {
    "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac"
   },
   "source": [
    "**Perform prediction on the prediction list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
   "metadata": {
    "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
    "outputId": "373a630c-9657-4555-d2e2-375addd84e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>701</th>\n",
       "      <th>702</th>\n",
       "      <th>703</th>\n",
       "      <th>704</th>\n",
       "      <th>705</th>\n",
       "      <th>706</th>\n",
       "      <th>707</th>\n",
       "      <th>708</th>\n",
       "      <th>709</th>\n",
       "      <th>710</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.74</td>\n",
       "      <td>0.72</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  711 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1     2     3     4    5     6     7     8     9    ...   701   702  \\\n",
       "0 -0.73  0.20 -0.62 -0.51  0.08  0.6 -0.31 -0.28 -0.75  0.24  ... -0.04 -0.74   \n",
       "1 -0.66  0.26 -0.27 -0.51  0.08  0.6 -0.31 -0.28 -0.75  0.24  ... -0.04 -0.74   \n",
       "2  0.11 -1.00 -0.96 -0.51  0.08  0.6 -0.31 -0.28 -0.75  0.24  ... -0.04 -0.74   \n",
       "3  0.24 -0.39 -0.04 -0.51  0.08  0.6 -0.31 -0.28 -0.75  0.24  ... -0.04 -0.74   \n",
       "4  0.76  0.85 -0.34 -0.51  0.08  0.6 -0.31 -0.28 -0.75  0.24  ... -0.04 -0.74   \n",
       "\n",
       "    703   704   705   706   707   708   709  710  \n",
       "0  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08  0.6  \n",
       "1  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08  0.6  \n",
       "2  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08  0.6  \n",
       "3  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08  0.6  \n",
       "4  0.72 -0.16  0.97  0.66 -0.16 -0.51  0.08  0.6  \n",
       "\n",
       "[5 rows x 711 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute amino acid features for 20 sequences in the dataset\n",
    "print(len(sequence))\n",
    "\n",
    "#X_df = mutants.head(100)\n",
    "X_pred = compute_features(mutants, 'mutant', 'ms_whim_scores')\n",
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c31c5a-d108-4fea-86ac-1cf0462bca29",
   "metadata": {},
   "source": [
    "**Make prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "656074e3-9912-4621-9c38-477cb54cfe71",
   "metadata": {
    "id": "656074e3-9912-4621-9c38-477cb54cfe71",
    "outputId": "c3f25706-92fd-4826-8f50-bbf41283c269"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3059</th>\n",
       "      <td>V162A</td>\n",
       "      <td>4.601901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3064</th>\n",
       "      <td>V162G</td>\n",
       "      <td>4.203137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>E4C</td>\n",
       "      <td>4.166193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3071</th>\n",
       "      <td>V162P</td>\n",
       "      <td>4.152483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>S71A</td>\n",
       "      <td>4.103170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  pred_score\n",
       "3059  V162A    4.601901\n",
       "3064  V162G    4.203137\n",
       "58      E4C    4.166193\n",
       "3071  V162P    4.152483\n",
       "1330   S71A    4.103170"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "X_pred_scaled = scaler.transform(X_pred)\n",
    "\n",
    "y_pred = h_estimator.predict(X_pred_scaled)\n",
    "results = pd.concat([mutants[['id']], pd.DataFrame(y_pred, columns=['pred_score'])], axis=1)\n",
    "results = results.sort_values(by='pred_score', ascending=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f359dd79-2e13-4144-8e10-72e90f2a87b3",
   "metadata": {
    "id": "f359dd79-2e13-4144-8e10-72e90f2a87b3"
   },
   "source": [
    "It is also possible to combine features from two different amino acid descriptors, which might (but not necessarily) improve performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4ef1d1fc-5ff2-4f6c-9176-bab300ab30fa",
   "metadata": {
    "id": "4ef1d1fc-5ff2-4f6c-9176-bab300ab30fa",
    "outputId": "d9b99379-e6df-40ea-933d-50f9de4b851f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "711 1185 1896\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1175</th>\n",
       "      <th>1176</th>\n",
       "      <th>1177</th>\n",
       "      <th>1178</th>\n",
       "      <th>1179</th>\n",
       "      <th>1180</th>\n",
       "      <th>1181</th>\n",
       "      <th>1182</th>\n",
       "      <th>1183</th>\n",
       "      <th>1184</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.73</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.62</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.66</td>\n",
       "      <td>0.26</td>\n",
       "      <td>-0.27</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.11</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>0.24</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.54</td>\n",
       "      <td>2.44</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.04</td>\n",
       "      <td>-1.47</td>\n",
       "      <td>2.29</td>\n",
       "      <td>0.89</td>\n",
       "      <td>-2.49</td>\n",
       "      <td>1.49</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1896 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0     1     2     3     4     5     6     7     8     9     ...  1175  \\\n",
       "0 -0.73  0.20 -0.62 -0.51  0.08   0.6 -0.31 -0.28 -0.75  0.24  ... -2.54   \n",
       "1 -0.66  0.26 -0.27 -0.51  0.08   0.6 -0.31 -0.28 -0.75  0.24  ... -2.54   \n",
       "2  0.11 -1.00 -0.96 -0.51  0.08   0.6 -0.31 -0.28 -0.75  0.24  ... -2.54   \n",
       "3  0.24 -0.39 -0.04 -0.51  0.08   0.6 -0.31 -0.28 -0.75  0.24  ... -2.54   \n",
       "4  0.76  0.85 -0.34 -0.51  0.08   0.6 -0.31 -0.28 -0.75  0.24  ... -2.54   \n",
       "\n",
       "   1176  1177  1178  1179  1180  1181  1182  1183  1184  \n",
       "0  2.44  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49  0.31  \n",
       "1  2.44  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49  0.31  \n",
       "2  2.44  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49  0.31  \n",
       "3  2.44  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49  0.31  \n",
       "4  2.44  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49  0.31  \n",
       "\n",
       "[5 rows x 1896 columns]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine two a.a. features\n",
    "Xm1 = compute_features(X_df, 'mutant', 'ms_whim_scores')\n",
    "Xm2 = compute_features(X_df, 'mutant', 'z_scales')\n",
    "X_paired = pd.concat([Xm1,Xm2], axis=1)\n",
    "print(len(Xm1.columns), len(Xm2.columns), len(X_paired.columns))\n",
    "X_paired.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "64265de4-1197-4d0e-a890-a89997a7b005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence    0     1    2     3  \\\n",
      "0  SKGEELFTGVVPILVELDGEVNGHKFSVSGEGEGDATYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "1  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "2  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "3  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDASYGKLTLKFI... -0.8  0.61 -1.0 -0.51   \n",
      "4  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGRLTLRFI... -0.8  0.61 -1.0 -0.51   \n",
      "\n",
      "      4    5     6     7     8  ...  1177  1178  1179  1180  1181  1182  1183  \\\n",
      "0  0.08  0.6 -0.31 -0.28 -0.75  ...  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49   \n",
      "1  0.08  0.6 -0.31 -0.28 -0.75  ...  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49   \n",
      "2  0.08  0.6 -0.31 -0.28 -0.75  ...  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49   \n",
      "3  0.08  0.6 -0.31 -0.28 -0.75  ...  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49   \n",
      "4  0.08  0.6 -0.31 -0.28 -0.75  ...  0.43  0.04 -1.47  2.29  0.89 -2.49  1.49   \n",
      "\n",
      "   1184     label  split  \n",
      "0  0.31  3.586350  train  \n",
      "1  0.31  3.612519  train  \n",
      "2  0.31  3.596791  train  \n",
      "3  0.31  3.572628  train  \n",
      "4  0.31  3.524680  train  \n",
      "\n",
      "[5 rows x 1899 columns]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# The previous model has been trained on 710 features. \n",
    "# To use another set of features with a different feature dimension, the model must be retrained.\n",
    "\n",
    "# Train and construct a new model for the new features\n",
    "\n",
    "# (1) Generate features from another descriptor\n",
    "X2 = compute_features(df, seqcol, 'z_scales')\n",
    "\n",
    "# (2) Combine two sets of amino acid descriptors\n",
    "newX = pd.concat([X,X2], axis=1)\n",
    "\n",
    "# (3) Combine columns\n",
    "cols = [n for n in df.columns if n != seqcol]\n",
    "newFeatures = pd.concat([df[seqcol], newX, df[cols]],axis=1)\n",
    "print(newFeatures.head())\n",
    "print(newFeatures.isna().values.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee97f9-f59e-4087-b339-b49064a1cf9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Define train set for training\n",
    "newTrain = newFeatures[newFeatures['split']=='train']\n",
    "newX_train = newTrain.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "newy_train = newTrain[ycol]\n",
    "\n",
    "# (5) Define test set for evaluation\n",
    "newTest = newFeatures[newFeatures['split']=='test']\n",
    "newX_test = newTest.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "newy_test = newTest[ycol]\n",
    "\n",
    "# (6) Transform the dataset\n",
    "newScaler = StandardScaler()\n",
    "newX_train_scaled = newScaler.fit_transform(newX_train)\n",
    "newX_test_scaled = newScaler.transform(newX_test)\n",
    "\n",
    "# (7) Perform hyperparameter optimization and construct the model\n",
    "newHyperopt, newH_estimator, newH_params = hyperopt_search(SVR(), param_grid, \n",
    "                                                           seed, newX_train_scaled, newX_test_scaled, \n",
    "                                                           newy_train, newy_test, n_trials, metric)\n",
    "\n",
    "print(f'Hyperopt HPO for paired dataset: {newHyperopt:.4f}')\n",
    "\n",
    "# (8) Make predictions\n",
    "X_paired_scaled = newScaler.transform(X_paired)\n",
    "y_paired = newH_estimator.predict(X_paired_scaled)\n",
    "results_p = pd.concat([X_df[['id']], pd.DataFrame(y_paired, columns=['pred_score'])], axis=1)\n",
    "results_p = results_p.sort_values(by='pred_score', ascending=False)\n",
    "print(results_p.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EmFVfXhWXJey",
   "metadata": {
    "id": "EmFVfXhWXJey"
   },
   "source": [
    "**References:** <br>\n",
    "* [Fluorescence TAPE benchmark dataset](https://github.com/songlab-cal/tape)\n",
    "* [Peptides - amino acid descriptors](https://peptides.readthedocs.io/en/stable/)\n",
    "* [scikit-learn.org](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "* [LazyPredict](https://lazypredict.readthedocs.io/en/latest/)\n",
    "* [PyCaret](https://github.com/pycaret/pycaret)\n",
    "* [AutoGluon](https://auto.gluon.ai/stable/tutorials/tabular/tabular-quick-start.html)\n",
    "* [MLJAR](https://supervised.mljar.com)\n",
    "* [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "* [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "* [Optuna](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.OptunaSearchCV.html)\n",
    "* [Hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "* [Skopt](https://scikit-optimize.github.io/stable/)\n",
    "* [Model selection: Nested Cross-Validation for Machine Learning with Python](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "* [Model selection: Training-validation-test split and cross-validation done right](https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/)\n",
    "* [Model construction: How to Train a Final Machine Learning Model](https://machinelearningmastery.com/train-final-machine-learning-model/)\n",
    "* [Model training: Embrace Randomness in Machine Learning](https://machinelearningmastery.com/randomness-in-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d261aaf-1e79-4804-aa9c-9f2ca6f6988c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
