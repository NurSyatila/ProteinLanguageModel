{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "76b9ed2b-d1ba-4373-869b-df23e60ef447",
   "metadata": {
    "id": "06baf6b6-c122-470e-a9da-bd30d0942c52"
   },
   "source": [
    "**Project:** Machine Learning Practice with Scikit-Learn\n",
    "\n",
    "**Goal:**\n",
    "This is a practice on machine-learning-guided protein engineering using Python scikit-learn and GFP dataset from [Saito et al. (2018)](https://doi.org/10.1021/acssynbio.8b00155). This page describes a step-by-step instruction where it should work on any local computer or supercomputer (with slight modifications).\n",
    "\n",
    "**Steps:**\n",
    "1. Data Collection\n",
    "2. Feature Extraction\n",
    "3. Preprocessing and Normalization\n",
    "4. Model Selection and Training\n",
    "5. Model Evaluation and Hyperparameter Tuning\n",
    "6. Model Prediction\n",
    "\n",
    "**Step 1: Data Collection**\n",
    "About the dataset (`../data/umetsu/Umetsu_GFP.csv`) : The dataset contains GFP variants, which include 153 variants from single-point or random multiple mutagenesis, reference GFP and reference YFP. The columns are: `Sequence`, `Intensity`, and `Change`. Sigmoidal function is used to generate the `Score` column.  \n",
    "\n",
    "`Sequence`: GFP variants<br>\n",
    "`Intensity`: Intensity<br>\n",
    "`Change`: Change to the intensity<br>\n",
    "\n",
    "**Prerequisites:**\n",
    "JupyterLab is an open-source web-based interactive development environment (IDE) primarily used for working with Jupyter notebooks, code, and data (advanced version of Jupyter Notebook). It is generally a good idea to install JupyterLab in a dedicated environment to avoid conflicts with other packages.\n",
    "\n",
    "To create a new conda environment, run:<br>\n",
    "\t&emsp;&emsp;&emsp;`conda create -n mlearn python=3.12`<br>\n",
    "\t&emsp;&emsp;&emsp;`conda activate mlearn`\n",
    "\n",
    "*(alternatively, create conda environment from the provided yml file: `conda env create -f mlearn.yml`)*<br>\n",
    "\n",
    "To install jupyer-lab, run:<br>\n",
    "    &emsp;&emsp;&emsp;`conda install -c conda-forge jupyterlab`\n",
    "\n",
    "Once JupyterLab is installed, you can start it by running:<br>\n",
    "    &emsp;&emsp;&emsp;`jupyter-lab`\n",
    "\n",
    "This will launch JupyterLab in your default web browser. It will typically open at `http://localhost:8888` (or another port if 8888 is already in use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3",
   "metadata": {
    "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3"
   },
   "outputs": [],
   "source": [
    "# When running in Google Colab\n",
    "# (i) Create a copy of data and notebook in your own drive\n",
    "# Mount Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "458425d1-434c-4e5d-a1f7-ba159cb0eda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Collecting peptides\n",
      "  Using cached peptides-0.5.0-py3-none-any.whl.metadata (49 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pandas) (2025.3)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.3.3-cp312-cp312-macosx_11_0_arm64.whl (10.7 MB)\n",
      "Using cached peptides-0.5.0-py3-none-any.whl (71 kB)\n",
      "Using cached scikit_learn-1.8.0-cp312-cp312-macosx_12_0_arm64.whl (8.1 MB)\n",
      "Using cached numpy-2.4.0-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached scipy-1.16.3-cp312-cp312-macosx_14_0_arm64.whl (20.9 MB)\n",
      "Using cached joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, peptides, numpy, joblib, scipy, pandas, scikit-learn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [scikit-learn][0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 numpy-2.4.0 pandas-2.3.3 peptides-0.5.0 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "# install required packages\n",
    "!pip install pandas peptides scikit-learn numpy scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e292d114-c9e6-4c60-819b-dfaed4a8b129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "# Check slearn version\n",
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef15ea9-f202-496b-adc1-590606c58ad0",
   "metadata": {},
   "source": [
    "**Step 2: Data processing**\n",
    "<br>Load the dataset and generate the `Score` column by running the sigmoidal function on `Intensity` and `Change` values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0739ff55-3ad2-415a-9bc8-f3a0eb73370b",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d8c05a2-99e0-400b-b92c-c37920f9ba04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence  Intensity    Change\n",
      "0     SSHT    1.00000  1.000000\n",
      "1     GAYF   10.36132  7.463609\n",
      "155\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_file = \"../data/umetsu/Umetsu_GFP.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "print(df.head(2))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ead9f-bca0-4a33-a9c4-bd03e61d3560",
   "metadata": {},
   "source": [
    "**Add the `Score` column based on sigmoidal function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3857b2ff-1c77-41c8-9a04-06b37f2523dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence  Intensity    Change     Score\n",
      "0     SSHT    1.00000  1.000000 -0.750000\n",
      "1     GAYF   10.36132  7.463609 -0.001643\n"
     ]
    }
   ],
   "source": [
    "# -0.75, -0.00164257656472999\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "thsh = 1.0\n",
    "\n",
    "df['Score'] = (\n",
    "    sigmoid(df['Intensity'] - thsh)\n",
    "    * sigmoid(df['Change'] - thsh)\n",
    "    - 1.0\n",
    ")\n",
    "\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88f714-48e7-49be-aefb-753677b26c6b",
   "metadata": {},
   "source": [
    "**Step 3: Generate amino acid features**\n",
    "<br>Each amino acid can be represented by a vector of physicochemical properties like hydrophobicity, charge, size, etc. A total of 8 different protein descriptor sets will be used in this study: Z-scales, VHSE, T-scales, ST-scales, MS-WHIM, FASGAI, BLOSUM and ProtFP ([van Westen et al., 2013](https://pmc.ncbi.nlm.nih.gov/articles/PMC3848949/)). Here, `peptides` Python package is used to compute descriptors for each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
   "metadata": {
    "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
    "outputId": "6f97fc86-d9f6-4680-9251-2acfd234cbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence      0     1     2     3     4     5     6     7     8  ...    11  \\\n",
      "0     SSHT  -7.44 -0.65  0.68 -0.17  1.58 -7.44 -0.65  0.68 -0.17  ... -1.31   \n",
      "1     GAYF -10.61 -1.21 -0.12  0.75  3.25 -9.11 -1.63  0.63  1.04  ... -0.47   \n",
      "\n",
      "     12    13    14    15    16    17    18    19     Score  \n",
      "0  0.01 -1.81 -0.21 -5.97 -0.62  1.11  0.31  0.95 -0.750000  \n",
      "1  0.07 -1.67 -0.35  0.49 -0.94 -0.63 -1.27 -0.44 -0.001643  \n",
      "\n",
      "[2 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "import peptides\n",
    "\n",
    "# Define a function to compute features, adding new columns\n",
    "# Available: blosum_indices fasgai_vectors ms_whim_scores protfp_descriptors st_scales t_scales vhse_scale z_scales\n",
    "def compute_features(df, seqcol, feature):\n",
    "    # Available features: available: blosum_indices fasgai_vectors ms_whim_scores protfp_descriptors st_scales t_scales vhse_scale z_scales\n",
    "    # and many more (refer to peptides package in github: https://github.com/althonos/peptides.py)\n",
    "\n",
    "    # Get features from a descriptor\n",
    "    data1=[[list(getattr(peptides.Peptide(a_a), feature)()) for a_a in list(seq)] for seq in df[seqcol]]\n",
    "    data2=[[s for j in k for s in j] for k in data1]\n",
    "\n",
    "    # Retrieve features (X) and target values (y)\n",
    "    X = pd.DataFrame(data2)\n",
    "    \n",
    "    return X\n",
    "\n",
    "# Define arguments\n",
    "RANDOM_STATE=0\n",
    "seqcol = 'Sequence'\n",
    "feature = 't_scales'\n",
    "\n",
    "# Get features for all sequences in the dataset (X)\n",
    "X = compute_features(df, seqcol, feature)\n",
    "\n",
    "# Combine the columns\n",
    "cols = [n for n in df.columns if n != seqcol]\n",
    "features = pd.concat([df[seqcol], X, df['Score']],axis=1)\n",
    "print(features.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d29bfa-9b1f-474f-9df1-47f3c75c0637",
   "metadata": {},
   "source": [
    "**Step 4: Model selection**\n",
    "<br>To identify which model is best used with the GFP dataset, several models are tested: `GaussianProcessRegressor()`, `Lasso()`, `RandomForestRegressor()` and `SVR()`.\n",
    "<br> The following steps are performed for individual model in `model_selection_*.py`:\n",
    "* Data loading: Load the data, `X = amino acid features (0:ncol-2)`, and `Y = Score (ncol-2)`\n",
    "* Data processing: Standardizes (`StandardScaler`) and transforms features (`fit_transform()`). The dataset is scaled based on mean and variance. Here, we use StandardScaler() that standardize the features so they have a mean of 0 and a standard deviation of 1.\n",
    "* Scoring metric: Define the metric used for calculating the performance score: `r2`, `rmse`, `pearson`, `spearman`\n",
    "* Initialize the selected model and search for best hyperparameters using grid search (`GridSearchCV()`)\n",
    "* Uses k-fold cross-validation (`KFold()` and `cross_validate()`) to evaluate model’s performance\n",
    "* Obtain best parameters and the performance score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43814405-7600-4e74-bd19-14e7a42ab09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, KFold, GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "\n",
    "# Define metric loading function\n",
    "def pearsonr_metric(y_true, y_pred):\n",
    "    r = pearsonr(x=y_true, y=y_pred)\n",
    "    return r[0] \n",
    "\n",
    "def spearmanr_metric(y_true, y_pred):\n",
    "    r = spearmanr(a=y_true, b=y_pred)\n",
    "    return r[0] \n",
    "\n",
    "def set_scoring(metric):\n",
    "    if metric == 'r2':\n",
    "        return 'r2'\n",
    "    elif metric == 'rmse':\n",
    "        return 'neg_root_mean_squared_error'\n",
    "    elif metric == 'pearson':\n",
    "        return make_scorer(pearsonr_metric)\n",
    "    elif metric == 'spearman':\n",
    "        return make_scorer(spearmanr_metric)\n",
    "    else:\n",
    "        print('wrong metric', metric)\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17d86a84-9855-4ea3-9882-17e3b1bdf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and labels\n",
    "X = features.iloc[:, 1:-1]\n",
    "y = features['Score']\n",
    "\n",
    "# Define metric\n",
    "metric = 'spearman'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018442f-8fc3-4ae8-8868-95d465dcbb81",
   "metadata": {},
   "source": [
    "**Model selection on SVR, Lasso and GPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b5d7ca-f5e9-481d-a054-67e40037418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to run model selection with nested CV\n",
    "import warnings\n",
    "    \n",
    "def model_selection(default_model, X, y, metric, RANDOM_STATE, param_grid=None):\n",
    "    # Define params\n",
    "    N_SPLITS=5\n",
    "\n",
    "    # Load metric\n",
    "    scoring = set_scoring(metric)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Define outer CV\n",
    "    outer_cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # ignores all warnings\n",
    "        \n",
    "        if param_grid: # run optimization if param_grid provided\n",
    "            # Only use nested (inner and outer) CV when optimization is performed\n",
    "            inner_cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "            model = GridSearchCV(estimator=default_model, param_grid=param_grid, cv=inner_cv, scoring=scoring, verbose=1) # inner CV\n",
    "            # Perform cross-validation with Kfold\n",
    "            res = cross_validate(estimator=model, X=X, y=y, cv=outer_cv, scoring=scoring, return_estimator=True, verbose=1) # outer CV\n",
    "            \n",
    "            if param_grid:\n",
    "                print('cv scores and parameters:')\n",
    "                for i in range(N_SPLITS):\n",
    "                    print(res['test_score'][i], res['estimator'][i].best_params_)\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            model = default_model\n",
    "            # Perform cross-validation with Kfold\n",
    "            res = cross_validate(estimator=model, X=X, y=y, cv=outer_cv, scoring=scoring, return_estimator=True, verbose=1) # outer CV\n",
    "            \n",
    "            print('cv scores:')\n",
    "            for i in range(N_SPLITS):\n",
    "                print(res['test_score'][i])\n",
    "        \n",
    "        print('mean score:')\n",
    "        print(res['test_score'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "782d5f5b-9aaa-446c-b320-079a241ebe6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (SVR) -- \n",
      "cv scores:\n",
      "0.44083942799449627\n",
      "0.6310900484738401\n",
      "0.719378995620727\n",
      "0.5255243547711439\n",
      "0.42597137568529864\n",
      "mean score:\n",
      "0.5485608405091011\n",
      "-- Optimized hyperparameters (SVR) -- \n",
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n",
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n",
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n",
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n",
      "cv scores and parameters:\n",
      "0.5123481409133896 {'C': 0.1, 'epsilon': 0.01, 'gamma': 0.1}\n",
      "0.6298310159581817 {'C': 10.0, 'epsilon': 0.0001, 'gamma': 0.1}\n",
      "0.7182777648582003 {'C': 0.01, 'epsilon': 0.1, 'gamma': 0.1}\n",
      "0.5459841461137357 {'C': 0.01, 'epsilon': 0.01, 'gamma': 0.01}\n",
      "0.42597137568529864 {'C': 0.1, 'epsilon': 0.01, 'gamma': 0.1}\n",
      "mean score:\n",
      "0.5664824887057611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.2s finished\n"
     ]
    }
   ],
   "source": [
    "# try for SVR\n",
    "from sklearn.svm import SVR\n",
    "model = SVR()\n",
    "grid = {'gamma': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1], 'C': [1e-2, 1e-1, 1e0, 1e1, 1e2], 'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1e0]}\n",
    "\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (SVR) -- \")\n",
    "model_selection(model, X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (SVR) -- \")\n",
    "model_selection(model, X, y, metric, RANDOM_STATE, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d89c7997-955b-4dc5-8abd-ecc4fe67961c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (Lasso) -- \n",
      "cv scores:\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "mean score:\n",
      "nan\n",
      "-- Optimized hyperparameters (Lasso) -- \n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "cv scores and parameters:\n",
      "0.45380529352374616 {'alpha': 0.01}\n",
      "0.5904862498438523 {'alpha': 0.01}\n",
      "0.727913534030311 {'alpha': 0.01}\n",
      "0.5109102180978641 {'alpha': 0.01}\n",
      "0.4250247726282202 {'alpha': 0.01}\n",
      "mean score:\n",
      "0.5416280136247987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "# try for Lasso\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(max_iter=100000)\n",
    "grid = {'alpha': [1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 5e-1, 1, 2, 5, 1e+1, 2e+1, 5e+1, 1e+2]}\n",
    "\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (Lasso) -- \")\n",
    "model_selection(Lasso(), X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (Lasso) -- \")\n",
    "model_selection(model, X, y, metric, RANDOM_STATE, grid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3c4b7df-e97d-4d27-9930-2b0267194376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (GPR) -- \n",
      "cv scores:\n",
      "-0.41019283674354196\n",
      "-0.2987054643399872\n",
      "-0.24997938309361661\n",
      "-0.22505770476850992\n",
      "-0.411772329829122\n",
      "mean score:\n",
      "-0.3191415437549555\n",
      "-- Optimized hyperparameters (GPR) -- \n",
      "cv scores:\n",
      "0.49781065531998825\n",
      "0.6304605322160108\n",
      "0.7075407649235625\n",
      "0.6547133229629379\n",
      "0.42597137568529864\n",
      "mean score:\n",
      "0.5832993302215596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# try for GPR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "# n_restarts_optimizer = different random starting points\n",
    "# it controls how many times optimizer restarts when fitting the kernel hyperparameters\n",
    "# thus no params will be used here for optimization\n",
    "\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (GPR) -- \")\n",
    "model_selection(GaussianProcessRegressor(), X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (GPR) -- \")\n",
    "model = GaussianProcessRegressor(n_restarts_optimizer=10, normalize_y=True, random_state=RANDOM_STATE)\n",
    "model_selection(model, X, y, metric, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b",
   "metadata": {
    "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b"
   },
   "source": [
    "**Step 5: Model Construction: Construct the final model that will be used for prediction**\n",
    "<br>The following steps are performed for individual model in `model_construction_*.py`:\n",
    "* Data loading: Load the data, `X = amino acid features (0:ncol-2)`, and `Y = Score (ncol-2)`\n",
    "* Data processing: Standardizes (`StandardScaler`) and transforms features (`fit_transform()`)\n",
    "* Scoring metric: Define the metric used for calculating the performance score: `r2`, `rmse`, `pearson`, `spearman`\n",
    "* Initialize the selected model and search for best hyperparameters using grid search (`GridSearchCV()`)\n",
    "* Uses k-fold cross-validation (`KFold()` and `cross_validate()`) to evaluate model’s performance\n",
    "* Train the model with training dataset (`fit()`)\n",
    "* Obtain best parameters (`.best_params_`) and the performance score (`.best_score_`)\n",
    "* Save the best estimator (`.best_estimator_`)  using `pickle` (not demonstrated here)\n",
    "\n",
    "Here, three models will be constructed: linear model `Lasso()`,  support vector machine `SVR()`, and gaussian model `GaussianProcessRegressor()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
   "metadata": {
    "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
    "outputId": "fe8d1b2c-74b8-4ed7-b62e-5fbb89c80923"
   },
   "outputs": [],
   "source": [
    "# Define function to run model construction nested CV\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "def model_construction(default_model, X, y, metric, RANDOM_STATE, param_grid=None):\n",
    "    # Define params\n",
    "    N_SPLITS=5\n",
    "\n",
    "    # Load metric\n",
    "    scoring = set_scoring(metric)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Define outer CV\n",
    "    outer_cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")  # ignores all warnings\n",
    "        \n",
    "        if param_grid: # run optimization if param_grid provided\n",
    "            # Only use nested (inner and outer) CV when optimization is performed\n",
    "            inner_cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "            model = GridSearchCV(estimator=default_model, param_grid=param_grid, cv=inner_cv, scoring=scoring, verbose=1) # inner CV\n",
    "            \n",
    "            # Cross validation will not be performed here\n",
    "            # Fit the best hyperparamater to construct updated model\n",
    "            model.fit(X=X, y=y)\n",
    "            \n",
    "            print('best parameter:', model.best_params_)\n",
    "            print('best score:', model.best_score_)\n",
    "\n",
    "            # get best estimator\n",
    "            bsmodel = model.best_estimator_\n",
    "\n",
    "        else:\n",
    "            bsmodel = default_model\n",
    "            \n",
    "            # Fit the default hyperparamater to construct updated model\n",
    "            bsmodel.fit(X=X, y=y)\n",
    "\n",
    "            # Evaluate the prediction\n",
    "            y_pred = bsmodel.predict(X)\n",
    "            if metric == 'r2':\n",
    "                val = r2_score(y, y_pred)\n",
    "            elif metric == 'rmse':\n",
    "                val = - np.sqrt(mean_squared_error(y, y_pred, squared=False))\n",
    "            elif metric == 'pearson':\n",
    "                val = pearsonr_metric(y, y_pred)\n",
    "            elif metric == 'spearman':\n",
    "                val = spearmanr_metric(y, y_pred)\n",
    "            else:\n",
    "                print('wrong metric', metric)\n",
    "                exit()    \n",
    "            \n",
    "            print('default score:', val)\n",
    "\n",
    "    return bsmodel, scaler\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc293932-96a1-44d8-8b59-5de7b717847c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (SVR) -- \n",
      "default score: 0.5862369793351073\n",
      "-- Optimized hyperparameters (SVR) -- \n",
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n",
      "best parameter: {'C': 0.1, 'epsilon': 0.01, 'gamma': 0.1}\n",
      "best score: 0.5854679124322926\n"
     ]
    }
   ],
   "source": [
    "# try for SVR\n",
    "model = SVR()\n",
    "svr_grid = {'gamma': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1], 'C': [1e-2, 1e-1, 1e0, 1e1, 1e2], 'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1e0]}\n",
    "\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (SVR) -- \")\n",
    "sdef_model, sdef_scaler = model_construction(model, X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (SVR) -- \")\n",
    "sbs_model, sbs_scaler = model_construction(model, X, y, metric, RANDOM_STATE, svr_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c8ad392-55f9-422f-a28a-30086c0c1630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (Lasso) -- \n",
      "default score: nan\n",
      "-- Optimized hyperparameters (Lasso) -- \n",
      "Fitting 5 folds for each of 13 candidates, totalling 65 fits\n",
      "best parameter: {'alpha': 0.01}\n",
      "best score: 0.5416280136247987\n"
     ]
    }
   ],
   "source": [
    "# try for Lasso\n",
    "model = Lasso(max_iter=100000)\n",
    "lasso_grid = {'alpha': [1e-2, 2e-2, 5e-2, 1e-1, 2e-1, 5e-1, 1, 2, 5, 1e+1, 2e+1, 5e+1, 1e+2]}\n",
    "\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (Lasso) -- \")\n",
    "ldef_model, ldef_scaler = model_construction(model, X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (Lasso) -- \")\n",
    "lbs_model, lbs_scaler = model_construction(model, X, y, metric, RANDOM_STATE, lasso_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdc3e590-8448-481a-9390-fbfd30246223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Default hyperparameters (GPR) -- \n",
      "default score: 0.6183705474834948\n",
      "-- Optimized hyperparameters (GPR) -- \n",
      "default score: 0.6183810096914947\n"
     ]
    }
   ],
   "source": [
    "# try for GPR\n",
    "# Run default parameters\n",
    "print(\"-- Default hyperparameters (GPR) -- \")\n",
    "gdef_model, gdef_scaler = model_construction(GaussianProcessRegressor(), X, y, metric, RANDOM_STATE)\n",
    "\n",
    "# Optimize hyperparameters\n",
    "print(\"-- Optimized hyperparameters (GPR) -- \")\n",
    "model = GaussianProcessRegressor(n_restarts_optimizer=10, normalize_y=True, random_state=RANDOM_STATE)\n",
    "gbs_model, gbs_scaler = model_construction(model, X, y, metric, RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9fc6c493-91ca-42c7-a2de-757afbbfd272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this to save the model for future use\n",
    "# pickle.dump(gbs_model, open(outsuf + \".model.pickle\", 'wb'))\n",
    "# pickle.dump(gbs_scaler, open(outsuf + \".scaler.pickle\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84b5eb6-90a4-491c-974d-005133688ef4",
   "metadata": {},
   "source": [
    "Here, GPR appears to outperform the other models. Therefore, we will use this model for prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5",
   "metadata": {
    "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5"
   },
   "source": [
    "**Step 6: Model Prediction**\n",
    "<br>After evaluating several models based on their performance metrics, the best model can be saved for prediction on unseen or real data. For example, the model can be used to predict the fluorescent intensity of variants, given the amino acid features as the input.\n",
    "<br>The following steps are performed for the saved model in `model_prediction_*.py`:\n",
    "* Data loading: Load the data, `X = amino acid features (0:ncol-2)`, and `Y = Score (ncol-2)`\n",
    "* Data processing: Standardizes (`StandardScaler`) and transforms features (`fit_transform()`)\n",
    "* Scoring metric: Define the metric used for calculating the performance score: `r2`, `rmse`, `pearson`, `spearman`\n",
    "* Load the selected model that had been previously saved  (`pickle.load()`)\n",
    "* Make prediction(`predict(X)`)\n",
    "* Print the performance score (`.best_score_`)\n",
    "* Print the predicted value (performance score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28777e3-fe60-4e65-893c-0f1b733f413d",
   "metadata": {
    "id": "c28777e3-fe60-4e65-893c-0f1b733f413d"
   },
   "source": [
    "**Load pre-computed dataset for prediction**\n",
    "<br>The dataset contains pre-computed T-scale features and score for a set of GFP mutants (n=160,000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe50bdc3-09b3-424b-8aba-32e1664cbe99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sequence      f   f.1   f.2   f.3   f.4   f.5   f.6   f.7   f.8  ...  f.11  \\\n",
      "0     SSHT  -7.44 -0.65  0.68 -0.17  1.58 -7.44 -0.65  0.68 -0.17  ... -1.31   \n",
      "1     GAYF -10.61 -1.21 -0.12  0.75  3.25 -9.11 -1.63  0.63  1.04  ... -0.47   \n",
      "\n",
      "   f.12  f.13  f.14  f.15  f.16  f.17  f.18  f.19     Score  \n",
      "0  0.01 -1.81 -0.21 -5.97 -0.62  1.11  0.31  0.95 -0.750000  \n",
      "1  0.07 -1.67 -0.35  0.49 -0.94 -0.63 -1.27 -0.44 -0.001643  \n",
      "\n",
      "[2 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "pred_file = \"../data/umetsu/Umetsu_GFP_T-scale_pred.csv\"\n",
    "pred_df = pd.read_csv(pred_file)\n",
    "print(pred_df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c62c7b1-f7f3-4c84-8350-6b2f530bd419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       f   f.1   f.2   f.3   f.4   f.5   f.6   f.7   f.8   f.9  f.10  f.11  \\\n",
      "0  -7.44 -0.65  0.68 -0.17  1.58 -7.44 -0.65  0.68 -0.17  1.58 -1.01 -1.31   \n",
      "1 -10.61 -1.21 -0.12  0.75  3.25 -9.11 -1.63  0.63  1.04  2.26  2.08 -0.47   \n",
      "\n",
      "   f.12  f.13  f.14  f.15  f.16  f.17  f.18  f.19  \n",
      "0  0.01 -1.81 -0.21 -5.97 -0.62  1.11  0.31  0.95  \n",
      "1  0.07 -1.67 -0.35  0.49 -0.94 -0.63 -1.27 -0.44  \n",
      "0   -0.750000\n",
      "1   -0.001643\n",
      "Name: Score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X = pred_df.iloc[:, 1:-1]\n",
    "y = pred_df['Score']\n",
    "print (X.head(2))\n",
    "print(y.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac",
   "metadata": {
    "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac"
   },
   "source": [
    "**Perform prediction on the prediction list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
   "metadata": {
    "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
    "outputId": "373a630c-9657-4555-d2e2-375addd84e1d"
   },
   "outputs": [],
   "source": [
    "# Define a function for model prediction\n",
    "\n",
    "def model_prediction(df, model, scaler, metric):\n",
    "    \n",
    "    # Define X and y here\n",
    "    X = df.iloc[:, 1:-1]\n",
    "    y = df['Score']\n",
    "    \n",
    "    # Transform the features (do not perform fit_transform so that it remain unseen)\n",
    "    # Scaler must be the same scaler used for training and hyperparameter optimization\n",
    "    X_scaled = scaler.transform(X)\n",
    "    \n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_scaled)\n",
    "\n",
    "    # Evaluate the prediction\n",
    "    if metric == 'r2':\n",
    "        val = r2_score(y, y_pred)\n",
    "    elif metric == 'rmse':\n",
    "        val = - np.sqrt(mean_squared_error(y, y_pred, squared=False))\n",
    "    elif metric == 'pearson':\n",
    "        val = pearsonr_metric(y, y_pred)\n",
    "    elif metric == 'spearman':\n",
    "        val = spearmanr_metric(y, y_pred)\n",
    "    else:\n",
    "        print('wrong metric', metric)\n",
    "        exit()\n",
    "    \n",
    "    print('score:', val)\n",
    "\n",
    "    # Create a dataframe and sort\n",
    "    pred = pd.concat([df['Sequence'], df['Score'], pd.DataFrame(y_pred, columns=['Pred'])], axis=1)\n",
    "    \n",
    "    return (pred)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c31c5a-d108-4fea-86ac-1cf0462bca29",
   "metadata": {},
   "source": [
    "**Make prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39e5e724-2919-4cf1-8534-adeb8ec5e79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.037923251242282005\n",
      "score: -0.001856800304612085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages/sklearn/utils/validation.py:2684: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.014895402756190319\n"
     ]
    }
   ],
   "source": [
    "g_pred = model_prediction(pred_df, gbs_model, gbs_scaler, metric)\n",
    "l_pred = model_prediction(pred_df, lbs_model, lbs_scaler, metric)\n",
    "s_pred = model_prediction(pred_df, sbs_model, sbs_scaler, metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5923129-75ec-4a9c-a470-a10414ca8e43",
   "metadata": {},
   "source": [
    "The evaluation score seems to be very poor, but this is expected considering large data points in the prediction list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "656074e3-9912-4621-9c38-477cb54cfe71",
   "metadata": {
    "id": "656074e3-9912-4621-9c38-477cb54cfe71",
    "outputId": "c3f25706-92fd-4826-8f50-bbf41283c269"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Score</th>\n",
       "      <th>Pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GAYF</td>\n",
       "      <td>-0.001643</td>\n",
       "      <td>-0.001643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCFV</td>\n",
       "      <td>-0.224490</td>\n",
       "      <td>-0.224490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASSV</td>\n",
       "      <td>-0.319873</td>\n",
       "      <td>-0.319873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GSHT</td>\n",
       "      <td>-0.330087</td>\n",
       "      <td>-0.330087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56276</th>\n",
       "      <td>GAHF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.330359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SSHY</td>\n",
       "      <td>-0.372963</td>\n",
       "      <td>-0.372963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56376</th>\n",
       "      <td>GAFF</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.392235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56480</th>\n",
       "      <td>GAYY</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.407431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56471</th>\n",
       "      <td>GAYH</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.417859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>TSHT</td>\n",
       "      <td>-0.418958</td>\n",
       "      <td>-0.418958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sequence     Score      Pred\n",
       "1         GAYF -0.001643 -0.001643\n",
       "2         CCFV -0.224490 -0.224490\n",
       "7         ASSV -0.319873 -0.319873\n",
       "20        GSHT -0.330087 -0.330087\n",
       "56276     GAHF  1.000000 -0.330359\n",
       "13        SSHY -0.372963 -0.372963\n",
       "56376     GAFF  1.000000 -0.392235\n",
       "56480     GAYY  1.000000 -0.407431\n",
       "56471     GAYH  1.000000 -0.417859\n",
       "22        TSHT -0.418958 -0.418958"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get top 10 mutants with best scores\n",
    "\n",
    "results = g_pred.sort_values(by='Pred', ascending=False)\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EmFVfXhWXJey",
   "metadata": {
    "id": "EmFVfXhWXJey"
   },
   "source": [
    "**References:** <br>\n",
    "* [Fluorescence TAPE benchmark dataset](https://github.com/songlab-cal/tape)\n",
    "* [Peptides - amino acid descriptors](https://peptides.readthedocs.io/en/stable/)\n",
    "* [scikit-learn.org](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "* [Model selection: Nested Cross-Validation for Machine Learning with Python](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "* [Model selection: Training-validation-test split and cross-validation done right](https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/)\n",
    "* [Model construction: How to Train a Final Machine Learning Model](https://machinelearningmastery.com/train-final-machine-learning-model/)\n",
    "* [Model training: Embrace Randomness in Machine Learning](https://machinelearningmastery.com/randomness-in-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d261aaf-1e79-4804-aa9c-9f2ca6f6988c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
