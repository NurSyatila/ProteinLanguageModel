{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc26e51b-d484-4442-98ca-9e62ab53bd39",
   "metadata": {},
   "source": [
    "**Project:** Protein Function Improvement through MLM-based Finetuning of Protein Language Models\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "The project focuses on improving binding affinity predictions using protein language models (PLMs). This project utilizes several protein language models (like ESM2, ProtBERT, or TAPE). Protein language models (like ESM2, ProtBERT, TAPE, or BERT-based models) are deep learning models trained on vast datasets of protein sequences. These models represent a protein sequence in the form of embeddings that represent both local and global patterns in the protein sequence.\n",
    "\n",
    "The goal of this project is to predict protein variants with higher fluorescence intensity based on amino acid sequences. A similar workflow can be beneficial for predicting other protein function properties.\n",
    "\n",
    "**Finetuning** is a type of transfer learning, i.e., a process in which a pre-trained protein language model (e.g. ESM2) is further trained or adapted on a specific, smaller dataset for a more specialized task.\n",
    "\n",
    "Masked language modeling (MLM) is a self-supervised learning adapted based on Natural Language Processing (NLP). In the case of PLM, MLM works as follows:\n",
    "- A protein sequence is treated as a sentence\n",
    "- Amino acid are treated as tokens\n",
    "- A fraction of amino acids (eg. 15%) are masked\n",
    "- The model is trained to predict the masked amino acids using the surrounding sequence context\n",
    "\n",
    "Such learning objective help the model to learns local sequence motifs, long-range dependencies, evolutionary constraints, and structural or functional patterns encoded in sequences.\n",
    "\n",
    "In this study, we will explore the utilities of masked language modeling task for fine-tuning the PLM using homologous sequences as the input sequences for training. Since homologous sequences tend to have conserved structural and/or functional properties, the model can be trained to adapt and recognize potential functional residues or evolutionary-associated information in the sequences.\n",
    "\n",
    "What to expect?\n",
    "1. Performance of the model on donwstream supervised tasks improved\n",
    "2. Embeddings have richer information\n",
    "\n",
    "**Evaluation:** Perplexity is a metric that measures how well a model predicts a sequence. Lower perplexity indicate better predictive performance. For each masked token, the model predicts teh probability distribution over amino acids and compute the negative log-likelihood of the true token.\n",
    "\n",
    "**APPROACH 1: FULL-MODEL FINETUNING**<br>\n",
    "\n",
    "In this approach, instead of freezing the ESM2 model and using its embeddings, the entire model (including both the transformer layers and the downstream task layer) is fine-tuned on the labeled dataset. The model learns both protein sequence representations and how they relate to the task-specific labels.\n",
    "* This involves updating all the parameters of the model, including both the pre-trained transformer layers and the final classification/regression layer. For regression tasks in our case, the final layer would output continuous values, such as binding affinity or other real-valued predictions.\n",
    "* The model’s hyperparameters are tuned to achieve the best performance on the task.\n",
    "\n",
    "**APPROACH 2: LoRA-BASED FINETUNING**<br>\n",
    "\n",
    "LoRA (Low-Rank Adaptation) is a more recent method designed to make model finetuning more efficient by introducing trainable low-rank matrices in certain layers of the pre-trained model.\n",
    "* Instead of updating the entire set of model parameters, LoRA only trains a small number of parameters, significantly reducing computational costs while still achieving good performance.\n",
    "* In LoRA, the main model remains largely frozen. Only low-rank matrices are added to specific parts of the model (typically the attention layers or the feed-forward layers of the transformer).\n",
    "\n",
    "<br>Dataset: Deep Mutational Scanning (DMS) data for the parent green fluorescent protein (GFP) protein and its mutants, originally derived from Sarkisyan et al. (2016). Pre-computed data splits from Rao et al. (2020) will be used. The pre-computed data splits contain 'train', 'valid', and 'test' sets. Evaluation will be made using the 'test' set across all approaches. For testing purposes, only 10% of the 'train' set will be retained.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6eb939b-0980-46cd-947a-41e70cef018d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: optuna-integration in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (4.6.0)\n",
      "Requirement already satisfied: torch in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (2.9.1)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (4.57.3)\n",
      "Requirement already satisfied: datasets in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: bio in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: peft in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: evaluate in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (0.4.6)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: optuna in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from optuna-integration) (4.6.0)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (3.20.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: biopython>=1.80 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from bio) (1.86)\n",
      "Requirement already satisfied: gprofiler-official in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from bio) (1.0.0)\n",
      "Requirement already satisfied: mygene in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from bio) (3.2.2)\n",
      "Requirement already satisfied: pooch in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from bio) (1.8.2)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from peft) (7.2.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from peft) (1.12.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.61.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Using cached pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: biothings-client>=0.2.6 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from mygene->bio) (0.4.1)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from optuna->optuna-integration) (1.17.2)\n",
      "Requirement already satisfied: colorlog in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from optuna->optuna-integration) (6.10.1)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from optuna->optuna-integration) (2.0.45)\n",
      "Requirement already satisfied: Mako in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from alembic>=1.5.0->optuna->optuna-integration) (1.3.10)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /opt/homebrew/Caskroom/miniconda/base/envs/mlearn/lib/python3.12/site-packages (from pooch->bio) (4.5.1)\n",
      "Using cached matplotlib-3.10.8-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-macosx_11_0_arm64.whl (273 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.61.1-cp312-cp312-macosx_10_13_universal2.whl (2.9 MB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pillow-12.1.0-cp312-cp312-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.1.0 pyparsing-3.3.1\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "!pip install ipywidgets optuna-integration torch transformers datasets bio peft evaluate matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9df72f8-dbdc-4059-8b39-73ab44a13bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When running in Google Colab\n",
    "# (i) Create a copy of data and notebook in your own drive\n",
    "# Mount Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42920bc1-9e91-452f-9e30-ebdf9f0c9c44",
   "metadata": {},
   "source": [
    "**Step 2a: Data Preparation and Processing**\n",
    "<br>We will load the homologous sequences for training. Data splits will be created and formatted for further training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b359459-2c6e-4f76-946d-86a466cbbf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from Bio import SeqIO\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "# Define a function that tokenize the sequence\n",
    "def tokenize_function(example, tokenizer, seqcol):\n",
    "    return tokenizer(example[seqcol], max_length=1024, padding=True, truncation=True)\n",
    "\n",
    "# Load sequences from a fasta file (input)\n",
    "def load_fasta_to_dict(fasta_file):\n",
    "    # Load the sequences from fasta file\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta-blast\"):\n",
    "        sequences.append({\"id\": record.id, \"sequence\": str(record.seq)})\n",
    "\n",
    "    # Convert the sequences into a Dataset object \n",
    "    dataset = Dataset.from_dict({\"id\": [seq[\"id\"] for seq in sequences],\n",
    "                \"sequence\": [seq[\"sequence\"] for seq in sequences]})\n",
    "    return dataset\n",
    "\n",
    "# Split the data into train and test\n",
    "def split_data(dataset, test_size, seed):\n",
    "    # 80% train + valid, 20% test\n",
    "    train_test = dataset['train'].train_test_split(test_size=test_size, seed=seed)\n",
    "    # Split the 80% train + valid in half\n",
    "    train_valid = train_test['train'].train_test_split(test_size=test_size, seed=seed)\n",
    "    # gather everyone if you want to have a single DatasetDict\n",
    "    dataset_splits = DatasetDict({\n",
    "        'train': train_valid['train'],\n",
    "        'valid': train_valid['test'],\n",
    "        'test': train_test['test']})\n",
    "    return dataset_splits\n",
    "    \n",
    "# Load the data file\n",
    "def load_data(inp_path, seed, checkpoint, tokenizer):\n",
    "    # If the input path is a file \n",
    "    if os.path.isfile(inp_path):\n",
    "        print(\"Given input is a local file. Data splits will be created...\")\n",
    "        dataset = DatasetDict({'train': load_fasta_to_dict(inp_path)})\n",
    "        dataset_splits = split_data(dataset, 0.2, seed)\n",
    "    else:\n",
    "        print(\"Given input does not exist. Try loading from HuggingFace Hub..\")\n",
    "\n",
    "    # Create tokenized dataset\n",
    "    tokenized_dataset = dataset_splits.map(partial(tokenize_function, tokenizer=tokenizer, seqcol='sequence'), batched=True)\n",
    "        \n",
    "    return tokenized_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0654740-e0b3-4a7a-a82d-f72885ea3b30",
   "metadata": {},
   "source": [
    "**Step 2b: Model loading (with classification head)**\n",
    "* In this case, only ESM2 models will be used (tested on ESM2 8M model). AutoModelForMaskedLM (with MLM head) is an ESM model equipped with MLM head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3f972-7393-4117-9152-7f3a44358ae0",
   "metadata": {},
   "source": [
    "**Define a function to load the model with MLM head**\n",
    "<br>Some models are adapted with MLM head and can be loaded using AutoModelForMaskedLM. from HuggingFace.\n",
    "<br>Utilities to use LoRA adapter is included in this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f7c47a6-8f29-4551-a3b9-c62e08319233",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmModel, AutoModelForMaskedLM\n",
    "import peft\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, inject_adapter_in_model, LoraConfig\n",
    "\n",
    "\n",
    "def get_model_params(model):\n",
    "    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "    return str(params)\n",
    "\n",
    "def load_model(checkpoint, lora_r, lora_alpha, lora_weights, lora_dropout, num_labels, half_precision, full = False):\n",
    "    model = AutoModelForMaskedLM.from_pretrained(checkpoint)\n",
    "    \n",
    "    print('full model : ',get_model_params(model))\n",
    "    if full == True:\n",
    "        return model       \n",
    "    \n",
    "    #peft_config = LoraConfig(\n",
    "    #    r=4, lora_alpha=1, bias=\"all\", target_modules=[\"query\",\"key\",\"value\",\"dense\"]\n",
    "    #)\n",
    "    wdict={\"q\":\"query\", \"k\":\"key\", \"v\":\"value\", \"d\":\"dense\", \"o\":\"dense\"}\n",
    "    target_modules=[wdict[m] for m in list(lora_weights)]\n",
    "\n",
    "    peft_config = LoraConfig(\n",
    "        r=lora_r, lora_alpha=lora_alpha, bias=\"all\", target_modules=target_modules, lora_dropout=lora_dropout\n",
    "    )\n",
    "    model = inject_adapter_in_model(peft_config, model)\n",
    "    \n",
    "    # Unfreeze the prediction head\n",
    "    for (param_name, param) in model.lm_head.named_parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    print('lora model : ',get_model_params(model))\n",
    "       \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223e2a84-f429-4275-a82c-c6b40e8d9c40",
   "metadata": {},
   "source": [
    "**Step 2e: Model training**\n",
    "\n",
    "**Define main training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e62c64c7-2bee-4fea-b701-0e603f925ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import TrainingArguments, Trainer, set_seed, EarlyStoppingCallback, DataCollatorForLanguageModeling\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os, math\n",
    "\n",
    "# Set random seeds for reproducibility of trainings run\n",
    "def set_seeds(s):\n",
    "    torch.manual_seed(s)\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    set_seed(s)\n",
    "\n",
    "# Main training function\n",
    "def train(checkpoint, tokenizer, dataset, logpath=\".\",                  # model checkpoint\n",
    "        lora_r=4, lora_alpha=1, lora_weights='qkvo', lora_dropout=0.0,  # lora arguments\n",
    "        num_labels = 1, accum = 1, batch = 2, val_batch = 2,            # training arguments    \n",
    "        lr = 1e-5, epochs = 20, seed = 42,                              # training arguments\n",
    "        earlystop = False, full = False):                               # training arguments\n",
    "\n",
    "    # Set all random seeds\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    print(\"Model used:\", checkpoint, \"\\n\")\n",
    "\n",
    "    eval_mode = False #obtain initial accuracy/perplexity of the model on the test set before training/finetuning\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.empty_cache() \n",
    "        mixed = True\n",
    "    else: \n",
    "        mixed = False\n",
    "\n",
    "    # set True to use deepspeed\n",
    "    #deepspeed = False\n",
    "\n",
    "    # Conditionally create the list of callbacks\n",
    "    callbacks = []\n",
    "    if earlystop: \n",
    "        print(\"Early stopping implemented.\")\n",
    "        save_strategy = \"epoch\"\n",
    "        load_best_model_at_end=True\n",
    "        callbacks.append(EarlyStoppingCallback(early_stopping_patience=5))\n",
    "    else: \n",
    "        print(\"Early stopping NOT implemented.\")\n",
    "        save_strategy = \"no\"\n",
    "        load_best_model_at_end=False\n",
    "\n",
    "    if \"esm\" in checkpoint: \n",
    "        save_safetensors=True\n",
    "    else: \n",
    "        save_safetensors=False    \n",
    "    #steps_per_epoch = math.ceil(len(train_dataset) / batch) \n",
    "\n",
    "    # Huggingface Trainer arguments\n",
    "    args = TrainingArguments(\n",
    "        logpath,                                        # Output directory\n",
    "        eval_strategy = \"epoch\",                        # Evaluation strategy (evaluate at the end of each epoch)\n",
    "        logging_strategy = \"epoch\",                     # Logging strategy (log at the end of each epoch)\n",
    "        learning_rate=lr,                               # Base learning rate\n",
    "        per_device_train_batch_size=batch,              # Batch size for training\n",
    "        per_device_eval_batch_size=val_batch,           # Batch size for evaluation\n",
    "        gradient_accumulation_steps=accum,\n",
    "        num_train_epochs=epochs,                        # # Total number of training epochs\n",
    "        seed = seed,\n",
    "        fp16 = mixed,\n",
    "        save_strategy = save_strategy,                  # Save model for every epoch\n",
    "        metric_for_best_model = 'eval_loss',\n",
    "        load_best_model_at_end=load_best_model_at_end,\n",
    "        save_total_limit = 2,\n",
    "        warmup_steps=500,                               # Number of steps for learning rate warmup\n",
    "        lr_scheduler_type=\"cosine\",                     # Set the scheduler type to \"linear\", \"cosine\", \"constant\", or \"polynomial\"\n",
    "        save_safetensors=save_safetensors\n",
    "    )\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(checkpoint, lora_r, lora_alpha, lora_weights, lora_dropout, num_labels, mixed, full)\n",
    "    \n",
    "    # Metric definition for validation data\n",
    "    experiment_id=\"_\".join(logpath.split(\"/\")[-4:])\n",
    "    # Preprocess_logits_for_metrics\n",
    "    # A function that takes the raw output logits from the model and prepares them into a format suitable for calculating evaluation metrics\n",
    "    def preprocess_logits_for_metrics(logits, labels):\n",
    "        if isinstance(logits, tuple):\n",
    "            # Depending on the model and config, logits may contain extra tensors,\n",
    "            # like past_key_values, but logits always come first\n",
    "            logits = logits[0]\n",
    "        return logits.argmax(dim=-1)\n",
    "\n",
    "    metric = evaluate.load(\"accuracy\", experiment_id=experiment_id)\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "        # by preprocess_logits_for_metrics\n",
    "        labels = labels.reshape(-1)\n",
    "        preds = preds.reshape(-1)\n",
    "        mask = labels != -100\n",
    "        labels = labels[mask]\n",
    "        preds = preds[mask]\n",
    "        return metric.compute(predictions=preds, references=labels)\n",
    "\n",
    "    # Prepare data collator (randomly mask the tokens)\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "    # Trainer          \n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        args=args,\n",
    "        train_dataset=dataset['train'],\n",
    "        eval_dataset=dataset['valid'],\n",
    "        processing_class=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks = callbacks,\n",
    "    )    \n",
    "    \n",
    "    # Try get initial perplexity\n",
    "    init_result = trainer.evaluate(dataset['test'])\n",
    "    init_val = init_result['eval_accuracy']\n",
    "    # Perplexity: A measure of how well a language model predicts a sample\n",
    "    try:\n",
    "        init_perplexity = math.exp(init_result[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        init_perplexity = float(\"inf\")\n",
    "    print(f'initial perplexity: {init_val} ({init_perplexity})')\n",
    "    \n",
    "    # Train model\n",
    "    trainer.train()\n",
    "    # Load the test dataset\n",
    "\n",
    "    # Evaluate\n",
    "    test_result = trainer.evaluate(dataset['test'])\n",
    "    val = test_result['eval_accuracy']\n",
    "\n",
    "    # Perplexity: A measure of how well a language model predicts a sample\n",
    "    try:\n",
    "        perplexity = math.exp(test_result[\"eval_loss\"])\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    test_result[\"perplexity\"] = perplexity \n",
    "\n",
    "    # do not save model\n",
    "    #trainer.save_model(logpath)\n",
    "    #trainer.processing_class.save_pretrained(logpath)\n",
    "    updated_model = trainer.model\n",
    " \n",
    "    print(f\"test perplexity: {perplexity}\")\n",
    "    return updated_model, val, trainer.state.log_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8cf990-0dbd-43aa-af0a-1143c93620a2",
   "metadata": {},
   "source": [
    "**Train the PLM on default hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c9a59ad9-ea1f-4006-ba16-449ff551f141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n",
      "Given input is a local file. Data splits will be created...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b82a708beab4b0d9aeda27362661420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/467 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f08c42c189c44eb28e4660544d08ed94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/117 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7953993d4a13443bb434b82d1288242c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/147 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'sequence', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 467\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['id', 'sequence', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 117\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'sequence', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 147\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define arguments\n",
    "\n",
    "seed = 42 # random seed to use\n",
    "checkpoint=\"facebook/esm2_t6_8M_UR50D\" # model to use\n",
    "fasta_file = \"../data/fluorescence_homologs.fasta\" # homolog file\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "print('device:',device)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# Load data\n",
    "dataset = load_data(fasta_file, seed, checkpoint, tokenizer)\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc9a6b-1a4a-4bd9-8073-8ac083c3031d",
   "metadata": {},
   "source": [
    "**(i) Train the full model (FT-Full)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88fe4b7d-a01a-4f2c-a41e-d7adbbd1b2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 01:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.066400</td>\n",
       "      <td>6.924913</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.148776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.348700</td>\n",
       "      <td>4.854401</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.155927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.055800</td>\n",
       "      <td>3.812191</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.184594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.371800</td>\n",
       "      <td>3.279158</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.203932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.027500</td>\n",
       "      <td>2.927099</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.218876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.762900</td>\n",
       "      <td>2.711122</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.235480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.567700</td>\n",
       "      <td>2.555456</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.248025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.452800</td>\n",
       "      <td>2.465329</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.423557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.408300</td>\n",
       "      <td>2.454805</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.449473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.403400</td>\n",
       "      <td>2.446229</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.452229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 10.772250081667863\n",
      "Test accuracy (FT-Full): 0.44250805076719074\n",
      "Time taken: 123.6444799900055 sec.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#import logging\n",
    "#logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "\n",
    "# Train the model with default parameters (FT-Full)\n",
    "# Set smaller epochs for speed\n",
    "start = time.time()\n",
    "model1, val1, history1 = train(checkpoint, tokenizer, dataset, full=True, epochs=10)\n",
    "end = time.time()\n",
    "print(f\"Test accuracy (FT-Full): {val1}\")\n",
    "print(f\"Time taken: {(end-start)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7499c8-b792-410e-9f27-ba3e7f16c419",
   "metadata": {},
   "source": [
    "**(ii) Train the LoRA-adapted model (FT-LoRA)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee143b3a-12ed-4462-a390-d2124af97c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  276194\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.428000</td>\n",
       "      <td>8.148265</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.141749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.134000</td>\n",
       "      <td>7.754966</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.131333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.800500</td>\n",
       "      <td>7.196099</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.142956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.245600</td>\n",
       "      <td>6.397203</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.149929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5.587800</td>\n",
       "      <td>5.618222</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.142044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>5.102900</td>\n",
       "      <td>5.252876</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.143621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.779700</td>\n",
       "      <td>5.017852</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.128561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.606500</td>\n",
       "      <td>4.879914</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.132862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.574600</td>\n",
       "      <td>4.837505</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.132184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.568600</td>\n",
       "      <td>4.831002</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.135645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 94.39982639949626\n",
      "Test accuracy (FT-LoRA): 0.14737639704489486\n",
      "Time taken: 137.8727970123291 sec.\n"
     ]
    }
   ],
   "source": [
    "# Train the model with default parameters (FT-LoRA)\n",
    "start = time.time()\n",
    "model2, val2, history2 = train(checkpoint, tokenizer, dataset, full=False, epochs=10)\n",
    "end = time.time()\n",
    "print(f\"Test accuracy (FT-LoRA): {val2}\")\n",
    "print(f\"Time taken: {(end-start)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a0fe1-e6d7-4c71-ae9e-c24c3cc377b3",
   "metadata": {},
   "source": [
    "**(iii) Visualize the training logs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "be29a40b-4e78-4c34-ae2c-bf4aa2913675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize training log history\n",
    "def train_plot(history, remark): #pdf_output not defined here\n",
    "    # Get loss, val_loss, and the computed metric from history\n",
    "    loss = [x['loss'] for x in history if 'loss' in x]\n",
    "    val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "\n",
    "    # Get spearman (for regression) or accuracy value (for classification)\n",
    "    if [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x] != []:\n",
    "        metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "    else:\n",
    "        metric = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]\n",
    "\n",
    "    epochs = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "    # Get loss, val_loss, and the computed metric from history\n",
    "    loss = [x['loss'] for x in history if 'loss' in x]\n",
    "    val_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "\n",
    "    # Get spearman (for regression) or accuracy value (for classification)\n",
    "    if [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x] != []:\n",
    "        metric = [x['eval_spearmanr'] for x in history if 'eval_spearmanr' in x]\n",
    "    else:\n",
    "        metric = [x['eval_accuracy'] for x in history if 'eval_accuracy' in x]\n",
    "\n",
    "    epochs = [x['epoch'] for x in history if 'loss' in x]\n",
    "\n",
    "    fig, host = plt.subplots(figsize=(5,3), layout='constrained') #8,5\n",
    "    ax2 = host.twinx()\n",
    "    ax3 = host.twinx()\n",
    "    host.set_xlabel(\"Epoch\")\n",
    "    host.set_ylabel(\"Spearman\")\n",
    "    ax2.set_ylabel(\"Train loss\")\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel(\"Validation loss\")\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    p1 = host.plot(epochs, metric,    color='orange', label=\"Spearman\")\n",
    "    p2 = ax2.plot( epochs, loss,    color='red', label=\"Train loss\")\n",
    "    p3 = ax3.plot( epochs, val_loss, color='blue', label=\"Validation loss\")\n",
    "    ax3.spines['right'].set_position(('outward', 60))\n",
    "    host.yaxis.label.set_color(p1[0].get_color())\n",
    "    ax2.yaxis.label.set_color(p2[0].get_color())\n",
    "    ax3.yaxis.label.set_color(p3[0].get_color())\n",
    "\n",
    "    # Show the plot\n",
    "    plt.title(f'Training History ({remark})')\n",
    "    plt.show()\n",
    "    #plt.savefig(pdf_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7b9acbc-510f-4a4c-9c7d-0214d74bb706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_plot(history1[:-1], 'FT-Full') # remove the last log on test data evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bbadffd-88a6-42ea-b34e-3c8bf00e5072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_plot(history2[:-1], 'FT-LoRA') # remove the last log on test data evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b88531a-ac37-402c-9ec3-bb0560803022",
   "metadata": {},
   "source": [
    "LoRA introduces an efficient method to adapt pre-trained models by adding low-rank matrices to the original weights. Instead of updating the entire model, LoRA adds a small number of parameters that are learned during fine-tuning, and it modifies only these low-rank components while leaving the original weights of the pre-trained model fixed. This drastically reduces the number of trainable parameters, which sometimes results in better model adaptability and, in certain cases, less computation and faster training.\n",
    "\n",
    "In this case, the use of Low-Rank Adaptation (LoRA) did not lead to performance improvements compared to full fine-tuning. Specifically, the fully fine-tuned model achieved better results than the LoRA-adapted model when using default LoRA hyperparameters, including the rank, scaling factor (α), and selected target modules.\n",
    "\n",
    "Further optimization of LoRA-specific hyperparameters such as increasing the rank, adjusting the α scaling factor, or expanding the set of target modules, may help close the performance gap relative to full fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0482b80-2417-4386-acfb-20259872c61e",
   "metadata": {},
   "source": [
    " **Step 2f: Hyperparameter optimization**<br>\n",
    " Hyperparameter optimization can be performed using Optuna, which automate the search for the best set of hyperparameters by exploring the hyperparameter space and minimizing (or maximizing) a given objective function based on Bayesian optimization.<br>\n",
    "\n",
    " In the case of LoRA-based finetuning, LoRA hyperparameters (HPs) are a big factor that \n",
    " * r (lora_r): Rank of the low-rank matrix used in the adapter layers that describe the number of parameters used for the adaptation layer.\n",
    "* alpha (lora_alpha): Scaling factor for the low-rank adaptation.\n",
    "* Target modules (lora_weights): Target modules for which adaptation will be implemented.\n",
    "\n",
    "These HPs must be first considered for optimization as these HPs will determine the model design which affect the entire performance. The next step will be to optimize other HPs to further improve the performance based on best defined loRA HPs.\n",
    "* Learning rate (lr): It controls how much the model's weights are adjusted during each step of training based on the gradients calculated by backpropagation. For example, a lower learning rate makes the model update more slowly, which can lead to more stable convergence but might require more epochs to reach optimal performance\n",
    "* Batch size (batch): The number of samples processed in one forward and backward pass during training\n",
    "* Gradient accumulation (accum): A technique used to simulate a larger batch size without requiring more memory. This allows for training on effective larger batch size, e.g. `batch_size * gradient_accumulation_steps`\n",
    "* Validation batch size (val_batch): The number of samples that will be used in each batch during the validation phase.\n",
    "\n",
    "Here, Optuna will be used to optimize several hyperparameters: <br>\n",
    "Step 1: Optimize LoRA hyperparamaters <br>\n",
    "Step 2: Optimize other training hyperparameters <br>\n",
    "\n",
    "**(i) Define the Hyperparameter Search Space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c4b8e070-eea7-4709-868e-4ba216ad1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from functools import partial\n",
    "\n",
    "def objective(trial, checkpoint, tokenizer, dataset, logpath, \n",
    "              lora_r, lora_alpha, lora_weights, lora_dropout, \n",
    "              num_labels, accum, batch, val_batch, \n",
    "              lr, epochs, seed, earlystop, full, mode):\n",
    "    \n",
    "    # Define hyperparameters to tune\n",
    "    if mode=='lora':\n",
    "        lora_r = trial.suggest_int('lora_r', 1, 64, step=1)\n",
    "        lora_alpha = trial.suggest_int('lora_alpha', 1, 120, step=1)\n",
    "        lora_weights = trial.suggest_categorical('lora_weights', \n",
    "                                                 ['qkvo','q','k','v','o','qk','qv',\n",
    "                                                  'qo','kv','ko','vo','qkv','qko','qvo','kvo'])\n",
    "        \n",
    "    if mode=='hp':\n",
    "        lr = trial.suggest_float('lr', 1e-7, 1e-2, log=True)\n",
    "        #batch_size = trial.suggest_int('batch_size', 2, 128, step=2)\n",
    "        #val_size = trial.suggest_int('val_size', 2, 128, step=2)\n",
    "        # only learning rate will be optimized here \n",
    "\n",
    "    TrainArgs=[checkpoint, tokenizer, dataset, logpath, \n",
    "              lora_r, lora_alpha, lora_weights, lora_dropout, \n",
    "              num_labels, accum, batch, val_batch, \n",
    "              lr, epochs, seed, earlystop, full]\n",
    "    \n",
    "    model, val, history = train(*TrainArgs) \n",
    "    \n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bd36d7-d8f0-4d0b-b6dd-7e03cb861435",
   "metadata": {},
   "source": [
    "**(ii) Define the Optuna Study Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e24dfc0c-f575-465d-97c8-defff76c96fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(checkpoint, tokenizer, dataset, logpath,\n",
    "              lora_r, lora_alpha, lora_weights, lora_dropout, \n",
    "              num_labels, accum, batch, val_batch, \n",
    "              lr, epochs, seed, earlystop, full, mode, n_trials):\n",
    "    # Hyperparameters to tune\n",
    "\n",
    "    if mode=='lora':\n",
    "        objective_with_model = partial(objective, \n",
    "              checkpoint=checkpoint, tokenizer=tokenizer, dataset=dataset, logpath=logpath, \n",
    "              lora_r=None, lora_alpha=None, lora_weights=None, lora_dropout=lora_dropout, \n",
    "              num_labels=num_labels, accum=accum, batch=batch, val_batch=val_batch, \n",
    "              lr=lr, epochs=epochs, seed=seed, earlystop=earlystop, full=full, mode='lora')\n",
    "    if mode=='hp':\n",
    "        objective_with_model = partial(objective, \n",
    "              checkpoint=checkpoint, tokenizer=tokenizer, dataset=dataset, logpath=logpath,\n",
    "              lora_r=lora_r, lora_alpha=lora_alpha, lora_weights=lora_weights, lora_dropout=lora_dropout, \n",
    "              num_labels=num_labels, accum=accum, batch=batch, val_batch=val_batch, \n",
    "              lr=None, epochs=epochs, seed=seed, earlystop=earlystop, full=full, mode='hp')\n",
    "    study = optuna.create_study(direction='maximize', sampler = TPESampler(seed=seed))\n",
    "    study.optimize(objective_with_model, n_trials=n_trials)\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1739c7-808c-4964-b009-c75c19fdaa7c",
   "metadata": {},
   "source": [
    "**(iii) Define default hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb026deb-2906-4007-a0e2-7022777bcd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "logpath=\".\"\n",
    "lora_r=4\n",
    "lora_alpha=1\n",
    "lora_weights='qkvo'\n",
    "lora_dropout=0.0\n",
    "num_labels = 1\n",
    "accum = 1\n",
    "batch = 2\n",
    "val_batch = 2\n",
    "lr = 1e-5\n",
    "epochs = 10\n",
    "seed = 42\n",
    "earlystop = False\n",
    "n_trials = 3 # would be helpful with higher trials\n",
    "\n",
    "def compiled_args(lora_r, lora_alpha, lora_weights, lr, full):\n",
    "    TrainArgs=[checkpoint, tokenizer, dataset, logpath, \n",
    "               lora_r, lora_alpha, lora_weights, lora_dropout, \n",
    "               num_labels, accum, batch, val_batch,\n",
    "               lr, epochs, seed, earlystop, full]\n",
    "    return TrainArgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f59c07-ab83-4942-abac-ce0a0cca2984",
   "metadata": {},
   "source": [
    "**(iv) Optimize FT-Full**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caba8046-20e7-4784-b01f-eb516036a31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 16:39:41,152] A new study created in memory with name: no-name-a3ef7c30-12cb-4a88-93ac-3ef86d5b65c9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 01:59, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.165800</td>\n",
       "      <td>7.436160</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.148534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.685700</td>\n",
       "      <td>5.327202</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.142892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.480600</td>\n",
       "      <td>4.283880</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.168170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.803200</td>\n",
       "      <td>3.767071</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.190431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.463200</td>\n",
       "      <td>3.400983</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.219700</td>\n",
       "      <td>3.226390</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.211422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.031300</td>\n",
       "      <td>3.094043</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.214747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.927100</td>\n",
       "      <td>3.002399</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.210836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.896500</td>\n",
       "      <td>3.008617</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.212883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.903000</td>\n",
       "      <td>2.990173</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.214437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 16:41:45,561] Trial 0 finished with value: 0.24512218223148324 and parameters: {'lr': 7.45934328572655e-06}. Best is trial 0 with value: 0.24512218223148324.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 17.60890300756497\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 01:58, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.763700</td>\n",
       "      <td>1.585483</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.524352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.751100</td>\n",
       "      <td>1.672599</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.503443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.018300</td>\n",
       "      <td>1.981076</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.407819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.098500</td>\n",
       "      <td>1.903457</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.434391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.082500</td>\n",
       "      <td>1.881649</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.447000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.998200</td>\n",
       "      <td>1.829458</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.457594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.840800</td>\n",
       "      <td>1.622778</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.526933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.679400</td>\n",
       "      <td>1.505720</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.556890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.560500</td>\n",
       "      <td>1.424548</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.581897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.506100</td>\n",
       "      <td>1.394531</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.579618</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 16:43:49,374] Trial 1 finished with value: 0.5538927827240008 and parameters: {'lr': 0.005669849511478858}. Best is trial 1 with value: 0.5538927827240008.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 4.498078509308864\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 13:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.256400</td>\n",
       "      <td>1.650004</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.555125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.453700</td>\n",
       "      <td>1.164318</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.664781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.165200</td>\n",
       "      <td>1.015481</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.712006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>0.835202</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.757461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.873500</td>\n",
       "      <td>0.800092</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.778516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.795600</td>\n",
       "      <td>0.765898</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.790036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.720900</td>\n",
       "      <td>0.712913</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.797702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.648800</td>\n",
       "      <td>0.690684</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.809187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.601600</td>\n",
       "      <td>0.657920</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.819684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.582500</td>\n",
       "      <td>0.671946</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.812928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 16:57:05,470] Trial 2 finished with value: 0.8115173328281872 and parameters: {'lr': 0.0004570563099801456}. Best is trial 2 with value: 0.8115173328281872.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 1.9363756047724259\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c5fa40b6-68dc-43b9-90c3-7f6bf5c59ffa)')' thrown while requesting HEAD https://huggingface.co/facebook/esm2_t6_8M_UR50D/resolve/main/config.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full model :  7512474\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 20:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.256400</td>\n",
       "      <td>1.650003</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.555125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.453700</td>\n",
       "      <td>1.164209</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.664535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.165100</td>\n",
       "      <td>1.016770</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.712006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.998700</td>\n",
       "      <td>0.838868</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.761724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.871400</td>\n",
       "      <td>0.804111</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.778990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.795000</td>\n",
       "      <td>0.756009</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.786148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.712400</td>\n",
       "      <td>0.711065</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.800575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.649400</td>\n",
       "      <td>0.698445</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.808009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.598200</td>\n",
       "      <td>0.663209</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.818966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.584200</td>\n",
       "      <td>0.677830</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.810569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 17:55]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 1.948309410467667\n",
      "Default: 0.44250805076719074\n",
      "Optuna: 0.8046978594430764\n"
     ]
    }
   ],
   "source": [
    "# Only learning rate will be optimized\n",
    "FTrainArgs = compiled_args(lora_r, lora_alpha, lora_weights, None, full=True)\n",
    "FStudy = optimize(*FTrainArgs, 'hp', n_trials)\n",
    "# Re-train with best params from Optuna\n",
    "FOptimArgs = compiled_args(lora_r, lora_alpha, lora_weights, FStudy.best_trial.params['lr'], full=True)\n",
    "Fmodel, Fscore, Fhistory = train(*FOptimArgs)\n",
    "print(f\"Default: {val1}\")\n",
    "print(f\"Optuna: {Fscore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d442a0-fe8e-4f51-990f-a6e8ca852e4f",
   "metadata": {},
   "source": [
    "**(iv) Optimize FT-LoRA**\n",
    "<br>Step 1: Optimize LoRA hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "171b34a8-ed61-4bd9-ae68-9caba50efe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:27:41,737] A new study created in memory with name: no-name-23c8bba5-8cd9-450c-bd22-36b04f6ee18a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  795874\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:04, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.318700</td>\n",
       "      <td>7.873718</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.147323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.597800</td>\n",
       "      <td>6.329542</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.135760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.076100</td>\n",
       "      <td>4.737383</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.136017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.095900</td>\n",
       "      <td>3.984466</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.138323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.678500</td>\n",
       "      <td>3.621230</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.148210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.423700</td>\n",
       "      <td>3.440952</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.144107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.264400</td>\n",
       "      <td>3.302261</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.148671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.165300</td>\n",
       "      <td>3.219440</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.148174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.138800</td>\n",
       "      <td>3.209210</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.145833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.129800</td>\n",
       "      <td>3.205394</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.146497</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:29:51,723] Trial 0 finished with value: 0.17086569426027656 and parameters: {'lora_r': 24, 'lora_alpha': 115, 'lora_weights': 'ko'}. Best is trial 0 with value: 0.17086569426027656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 22.13198758156918\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  1201634\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:10, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.342900</td>\n",
       "      <td>7.925803</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.146353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.803300</td>\n",
       "      <td>6.782212</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.137236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.423400</td>\n",
       "      <td>5.060185</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.140643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.370700</td>\n",
       "      <td>4.240526</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.139507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.863300</td>\n",
       "      <td>3.786687</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.141096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.563900</td>\n",
       "      <td>3.571244</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.135844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.372900</td>\n",
       "      <td>3.409853</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.138137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.254100</td>\n",
       "      <td>3.318597</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.137102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.219900</td>\n",
       "      <td>3.289803</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.143918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.214100</td>\n",
       "      <td>3.293619</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.137532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:32:06,968] Trial 1 finished with value: 0.1612047736313696 and parameters: {'lora_r': 34, 'lora_alpha': 52, 'lora_weights': 'kvo'}. Best is trial 0 with value: 0.17086569426027656.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 24.215741508732687\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.302700</td>\n",
       "      <td>7.847583</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.527800</td>\n",
       "      <td>6.210803</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.135022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.953800</td>\n",
       "      <td>4.615860</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.138099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.018700</td>\n",
       "      <td>3.902564</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.607700</td>\n",
       "      <td>3.535116</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.350200</td>\n",
       "      <td>3.339246</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.166700</td>\n",
       "      <td>3.181899</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.152741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.059100</td>\n",
       "      <td>3.100968</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.149117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.025200</td>\n",
       "      <td>3.075150</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.157088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.016300</td>\n",
       "      <td>3.077498</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.151451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:34:24,326] Trial 2 finished with value: 0.17541200985035044 and parameters: {'lora_r': 62, 'lora_alpha': 98, 'lora_weights': 'kvo'}. Best is trial 2 with value: 0.17541200985035044.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 19.81103204351065\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:11, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.302700</td>\n",
       "      <td>7.847583</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.527800</td>\n",
       "      <td>6.210803</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.135022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.953800</td>\n",
       "      <td>4.615860</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.138099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.018700</td>\n",
       "      <td>3.902564</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.607700</td>\n",
       "      <td>3.535116</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.350200</td>\n",
       "      <td>3.339246</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.148238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.166700</td>\n",
       "      <td>3.181899</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.152741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.059100</td>\n",
       "      <td>3.100970</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.149117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.025200</td>\n",
       "      <td>3.075153</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.157088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.016300</td>\n",
       "      <td>3.077501</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.151451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 19.81107927674806\n",
      "Default: 0.14737639704489486\n",
      "Optuna: 0.17541200985035044, best params: {'lora_r': 62, 'lora_alpha': 98, 'lora_weights': 'kvo'}\n"
     ]
    }
   ],
   "source": [
    "# LoRA HPs will be optimized\n",
    "LTrainArgs = compiled_args(None, None, None, lr, full=False)\n",
    "LStudy = optimize(*LTrainArgs, 'lora', n_trials)\n",
    "# Re-train with best params from Optuna\n",
    "LOptimArgs = compiled_args(LStudy.best_trial.params['lora_r'], \n",
    "                           LStudy.best_trial.params['lora_alpha'], \n",
    "                           LStudy.best_trial.params['lora_weights'],\n",
    "                           lr, full=False)\n",
    "Lmodel, Lscore, Lhistory = train(*LOptimArgs)\n",
    "print(f\"Default: {val2}\")\n",
    "print(f\"Optuna: {Lscore}, best params: {LStudy.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03d324b-6b21-4a5c-899a-c5425a4d61de",
   "metadata": {},
   "source": [
    "Step 2: Optimize learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1486a3f4-5b5a-4211-b88c-26f97f3e78d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:52:48,279] A new study created in memory with name: no-name-61c00ff4-e6e6-4520-ac72-28a284a81ad4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.334100</td>\n",
       "      <td>7.916074</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.147080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>6.778600</td>\n",
       "      <td>6.736371</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.139203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.446300</td>\n",
       "      <td>5.173018</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.144576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.484300</td>\n",
       "      <td>4.399817</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.147797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.004500</td>\n",
       "      <td>3.948336</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.147024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.718500</td>\n",
       "      <td>3.760504</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.133657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.542000</td>\n",
       "      <td>3.622444</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.139574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.435500</td>\n",
       "      <td>3.535900</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.142992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.407900</td>\n",
       "      <td>3.513572</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.143199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.402700</td>\n",
       "      <td>3.521823</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.136117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:55:06,348] Trial 0 finished with value: 0.1610153438151165 and parameters: {'lr': 7.45934328572655e-06}. Best is trial 0 with value: 0.1610153438151165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 29.177937176017654\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.897900</td>\n",
       "      <td>1.701458</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.502544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.312200</td>\n",
       "      <td>2.146587</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.357354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.352400</td>\n",
       "      <td>2.146107</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.343511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.245800</td>\n",
       "      <td>2.099603</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.371388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.223900</td>\n",
       "      <td>2.056026</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.376808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.197500</td>\n",
       "      <td>2.044797</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.383232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.192200</td>\n",
       "      <td>2.027713</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.385684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.177400</td>\n",
       "      <td>2.019777</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.387986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.151700</td>\n",
       "      <td>2.017721</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.395833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.138300</td>\n",
       "      <td>1.989764</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.398443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:57:23,605] Trial 1 finished with value: 0.3472248531918924 and parameters: {'lr': 0.005669849511478858}. Best is trial 1 with value: 0.3472248531918924.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 8.589957235935898\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.894700</td>\n",
       "      <td>2.167526</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.478556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.799300</td>\n",
       "      <td>1.426274</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.590507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.356400</td>\n",
       "      <td>1.132699</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.674763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.128800</td>\n",
       "      <td>0.931035</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.730459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.869547</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.756225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>0.833818</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.757959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.817600</td>\n",
       "      <td>0.786398</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.776155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.754800</td>\n",
       "      <td>0.788516</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.790106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.709200</td>\n",
       "      <td>0.738242</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.789511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.697100</td>\n",
       "      <td>0.736003</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.789809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2026-01-09 14:59:40,841] Trial 2 finished with value: 0.7781776851676454 and parameters: {'lr': 0.0004570563099801456}. Best is trial 2 with value: 0.7781776851676454.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 2.1228879402146403\n",
      "Model used: facebook/esm2_t6_8M_UR50D \n",
      "\n",
      "Early stopping NOT implemented.\n",
      "full model :  7512474\n",
      "lora model :  2079714\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='133' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial perplexity: 0.14156965828013518 (1712.7117398082123)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2340' max='2340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2340/2340 02:12, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.894700</td>\n",
       "      <td>2.167526</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.478556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.799300</td>\n",
       "      <td>1.426273</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.590507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.356400</td>\n",
       "      <td>1.133012</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.675226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.128800</td>\n",
       "      <td>0.931800</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.729749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.979900</td>\n",
       "      <td>0.869926</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.756699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.883800</td>\n",
       "      <td>0.835161</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.757230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.817500</td>\n",
       "      <td>0.787486</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.776155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.754700</td>\n",
       "      <td>0.788652</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.791519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.709400</td>\n",
       "      <td>0.738022</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.791667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.696900</td>\n",
       "      <td>0.736225</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.789573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='74' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [74/74 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test perplexity: 2.12216656604381\n",
      "Default: 0.14737639704489486\n",
      "Optuna (Step 1): 0.17541200985035044, best params: {'lora_r': 62, 'lora_alpha': 98, 'lora_weights': 'kvo'}\n",
      "Optuna (Step 2): 0.7783671149838984, best params: {'lr': 0.0004570563099801456}\n"
     ]
    }
   ],
   "source": [
    "# Learning rate will be optimized\n",
    "LTrainArgs2 = compiled_args(LStudy.best_trial.params['lora_r'], LStudy.best_trial.params['lora_alpha'], LStudy.best_trial.params['lora_weights'], None, full=False)\n",
    "LStudy2 = optimize(*LTrainArgs2, 'hp', n_trials)\n",
    "# Re-train with best params from Optuna\n",
    "LOptimArgs2 = compiled_args(LStudy.best_trial.params['lora_r'], LStudy.best_trial.params['lora_alpha'], LStudy.best_trial.params['lora_weights'], LStudy2.best_trial.params['lr'], full=False)\n",
    "Lmodel2, Lscore2, Lhistory2 = train(*LOptimArgs2)\n",
    "print(f\"Default: {val2}\")\n",
    "print(f\"Optuna (Step 1): {Lscore}, best params: {LStudy.best_trial.params}\")\n",
    "print(f\"Optuna (Step 2): {Lscore2}, best params: {LStudy2.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb924ad2-6046-4cdd-9d55-d8eec7b8976a",
   "metadata": {},
   "source": [
    "Hyperparameter optimization can greatly improve the performance, but with a high computational cost. For a complete analysis, consider increasing the number of epochs (`epochs`), using a wider range of hyperparameter searching space, and a higher number of Optuna trials ('`n_trials`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e18d3-4906-4416-b9ed-ef92d519ca8d",
   "metadata": {},
   "source": [
    "**References:** <br>\n",
    "* [Fluorescence TAPE benchmark dataset](https://github.com/songlab-cal/tape)\n",
    "* [ESM-2 models](https://github.com/facebookresearch/esm)\n",
    "* [Protein Language Model](https://huggingface.co/models?other=protein+language+model)\n",
    "* [Fine-tuning protein language models boosts predictions across diverse tasks](https://www.nature.com/articles/s41467-024-51844-2)\n",
    "* [Democratizing protein language models with parameter-efficient fine-tuning](https://pubmed.ncbi.nlm.nih.gov/38900798/)\n",
    "* [Model training using HuggingFace](https://huggingface.co/docs/transformers/en/main_classes/trainer)\n",
    "* [Optuna for hyperparameter optimization](https://optuna.readthedocs.io/en/v2.0.0/index.html)\n",
    "* [Fine-tune a masked language model](https://huggingface.co/learn/llm-course/en/chapter7/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
