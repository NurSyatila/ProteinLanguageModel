{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06baf6b6-c122-470e-a9da-bd30d0942c52",
   "metadata": {
    "id": "06baf6b6-c122-470e-a9da-bd30d0942c52"
   },
   "source": [
    "**Project:** Protein Function Improvement through Protein Language Model Embeddings using Machine Learning Approach\n",
    "\n",
    "**Goal:**\n",
    "The goal of this project is to predict fluorescence intensity from amino acid sequences. Here, the protein sequence is represented in the form embeddings (mean embeddings of residues in the sequence) extracted from Protein Language Model (PLM). The project involves training various models and selecting the most optimal one for further prediction. A similar workflow can be beneficial for predicting other protein function properties.\n",
    "\n",
    "**Embeddings from Protein Language Models:** These models are pre-trained with a large protein sequence database to understand the sequences and structures of proteins, mapping them into dense vector representations, or embeddings. Embeddings capture complex relationships between amino acids and can be used as features for downstream tasks like protein function prediction.\n",
    "\n",
    "**Steps:**\n",
    "1. Data Collection\n",
    "2. Feature Extraction\n",
    "3. Preprocessing and Normalization\n",
    "4. Model Selection and Training\n",
    "5. Model Evaluation and Hyperparameter Tuning\n",
    "6. Model Prediction\n",
    "\n",
    "**Step 1: Data Collection**\n",
    "<br>Dataset: Deep Mutational Scanning (DMS) data for the parent green fluorescent protein (GFP) protein and its mutants, originally derived from Sarkisyan et al. (2016). Pre-computed data splits from Rao et al. (2020) will be used. The pre-computed data splits contain 'train', 'valid', and 'test' sets. Evaluation will be made using the 'test' set across all approaches. For testing purposes, only 10% of the 'train' set will be retained.  \n",
    "\n",
    "`Protein sequence`: GFP variants<br>\n",
    "`Target value`: Fluorescence intensity<br>\n",
    "\n",
    "**File:** fluorescence.csv (use scripts/create_dataset.py to create dataset for use)\n",
    "<br>`wget http://s3.amazonaws.com/songlabdata/proteindata/data_raw_pytorch/fluorescence.tar.gz`\n",
    "<br>`tar -xzvf fluorescence.tar.gz`\n",
    "<br>`python scripts/create_dataset.py -d fluorescence -o fluorescence.csv -t 0.1 (use -t 1.0 to use full dataset)`\n",
    "<br>`rm -rf fluorescence`\n",
    "<br>`rm fluorescence.tar.gz`\n",
    "\n",
    "**Prerequisites:**\n",
    "JupyterLab is an open-source web-based interactive development environment (IDE) primarily used for working with Jupyter notebooks, code, and data (advanced version of Jupyter Notebook). It is generally a good idea to install JupyterLab in a dedicated environment to avoid conflicts with other packages.\n",
    "\n",
    "To create a new conda environment, run:<br>\n",
    "\t&emsp;&emsp;&emsp;`conda create -n mlearn python=3.12`<br>\n",
    "\t&emsp;&emsp;&emsp;`conda activate mlearn`\n",
    "\n",
    "*(alternatively, create conda environment from the provided yml file: `conda env create -f mlearn.yml`)*<br>\n",
    "\n",
    "To install jupyer-lab, run:<br>\n",
    "    &emsp;&emsp;&emsp;`conda install -c conda-forge jupyterlab`\n",
    "\n",
    "Once JupyterLab is installed, you can start it by running:<br>\n",
    "    &emsp;&emsp;&emsp;`jupyter-lab`\n",
    "\n",
    "This will launch JupyterLab in your default web browser. It will typically open at `http://localhost:8888` (or another port if 8888 is already in use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ad5ef-ef25-4479-bb12-f3e407a3426e",
   "metadata": {
    "id": "a79ad5ef-ef25-4479-bb12-f3e407a3426e"
   },
   "outputs": [],
   "source": [
    "# install required packages\n",
    "!pip install pandas scipy scikit-learn ipywidgets optuna-integration hyperopt scikit-optimize torch transformers\n",
    "\n",
    "!pip install lazypredict\n",
    "#!pip install autogluon\n",
    "#!pip install mljar-supervised\n",
    "#!pip install pycaret\n",
    "# note: compatibility issue might arise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d03909e4-ecad-40da-8b41-0da0b83b7338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement scikit-learn-intelex==2023.1.1 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for scikit-learn-intelex==2023.1.1\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# try to install scikit-learn-intelex for speed\n",
    "!pip install scikit-learn-intelex==2023.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3",
   "metadata": {
    "id": "b93d848b-d9e9-4cf0-93db-1f8c8daac2b3"
   },
   "outputs": [],
   "source": [
    "# When running in Google Colab\n",
    "# (i) Create a copy of data and notebook in your own drive\n",
    "# Mount Drive\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c88f714-48e7-49be-aefb-753677b26c6b",
   "metadata": {},
   "source": [
    "**Step 2: Data Preparation and Processing**\n",
    "<br>Use precomputed data splits (in this study) or perform data splits for training using train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "845d3f05-11fe-480c-a18d-799bb4292644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define arguments\n",
    "seqcol = 'sequence' # column to sequence\n",
    "ycol = 'label' # column to score/label\n",
    "seed = 42 # random seed to use\n",
    "test_size = 0.2 # test size for data splits\n",
    "checkpoint=\"facebook/esm2_t6_8M_UR50D\" # model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1095c8-6842-4259-8811-f78fc4e9bd9e",
   "metadata": {
    "id": "db1095c8-6842-4259-8811-f78fc4e9bd9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence     label  split\n",
      "0  SKGEELFTGVVPILVELDGEVNGHKFSVSGEGEGDATYGKLTLKFI...  3.586350  train\n",
      "1  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...  3.612519  train\n",
      "5173 2097 524 2552\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# (A) In the case where data split is pre-computed\n",
    "# Load training data\n",
    "# csv_file=\"/content/drive/MyDrive/Colab Notebooks/fluorescence.csv\"\n",
    "csv_file = \"../data/fluorescence.csv\"\n",
    "df = pd.read_csv(csv_file)\n",
    "print(df.head(2))\n",
    "print(len(df), len(df[df['split']=='train']), len(df[df['split']=='valid']), len(df[df['split']=='test']))\n",
    "\n",
    "# (B) In the case where the data split is not defined\n",
    "def get_data_split(df, test_size):\n",
    "    train, test = train_test_split(\n",
    "        df,\n",
    "        test_size=test_size,\n",
    "        random_state=seed,\n",
    "    )\n",
    "    train['split'] = 'train'\n",
    "    test['split'] = 'test'\n",
    "    df = pd.concat([train,test])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb37803-eb11-4b39-84a7-dc6a5f40a421",
   "metadata": {
    "id": "bdb37803-eb11-4b39-84a7-dc6a5f40a421"
   },
   "source": [
    "**Step 3: Embedding Extraction from Protein Language Model**\n",
    "<br>Protein language models (pLMs) generate high-dimensional embeddings of a protein sequence on a per-residue level. The embeddings capture complex relationships between amino acids and can be used as features for downstream tasks like binding affinity prediction. For per-protein level prediction, the mean residue embeddings are used. As an example, the smallest ESM2 model variant will be used, i.e. esm2_t6_8M_UR50D.\n",
    "\n",
    "The final embedding used for the downstream task often requires pooling, a method that concatenates the per-residue-level embeddings to form a fixed-length vectors of protein-level representation. Pooling choice could affect the downstream prediction tasks since the method determines what information is preserved or discarded from the sequence. Mean pooling captures the global, average signal across all residues, max pooling emphasizes the presence of critical residues or motifs, and sum-pooling aggregates residue-level information while preserving overall signal magnitude. Here, we evaluate the performance of mean, max and sum-pooled embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
   "metadata": {
    "id": "deb654e6-7a9c-4f9a-8694-58208a349345",
    "outputId": "6f97fc86-d9f6-4680-9251-2acfd234cbc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            sequence         0         1  \\\n",
      "0  SKGEELFTGVVPILVELDGEVNGHKFSVSGEGEGDATYGKLTLKFI... -0.060974  0.028259   \n",
      "1  SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI... -0.049561  0.031830   \n",
      "\n",
      "          2         3         4         5         6         7         8  ...  \\\n",
      "0  0.204102  0.161255  0.226074 -0.047546  0.001337 -0.094971 -0.064819  ...   \n",
      "1  0.196167  0.151001  0.227783 -0.049774 -0.007648 -0.097412 -0.060760  ...   \n",
      "\n",
      "        312       313       314       315       316       317       318  \\\n",
      "0 -0.122559  0.094543  0.136230 -0.171143 -0.141479  0.116638  0.022171   \n",
      "1 -0.113037  0.093323  0.136597 -0.165649 -0.146851  0.100525  0.024429   \n",
      "\n",
      "        319     label  split  \n",
      "0  0.034637  3.586350  train  \n",
      "1  0.030014  3.612519  train  \n",
      "\n",
      "[2 rows x 323 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "print('device:',device)\n",
    "\n",
    "# define a function to compute embedding (define the column to extract sequence)\n",
    "def compute_embeddings(df, seq_col, checkpoint, pool):\n",
    "    # load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "    model = AutoModel.from_pretrained(checkpoint, dtype=torch.float16)\n",
    "    model = model.to(device)\n",
    "    if device.type == 'cuda':\n",
    "        model = model.half()\n",
    "        \n",
    "    all_embeddings = []\n",
    "    for i in range(0,len(df[seq_col])):\n",
    "        inputs = tokenizer(df[seq_col].loc[i], return_tensors=\"pt\", max_length = 1024, truncation=True, padding=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            # get pooled embeddings\n",
    "            outputs = model(**inputs).last_hidden_state.cpu()\n",
    "            if pool == 'mean':\n",
    "                embedding = torch.mean(outputs, dim=1)\n",
    "            elif pool == 'sum':\n",
    "                embedding = torch.sum(outputs, dim=1)\n",
    "            elif pool == 'max':\n",
    "                embedding, _ = torch.max(outputs, dim=1)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported pooling method: {pool} (mean, max, sum)\")\n",
    "            all_embeddings.append( embedding )\n",
    "    all_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    X = pd.DataFrame(all_embeddings.numpy())\n",
    "    return X\n",
    "\n",
    "# Get features for all sequences in the dataset (X)\n",
    "X = compute_embeddings(df, 'sequence', checkpoint, 'mean')\n",
    "\n",
    "# Combine the columns\n",
    "cols = [n for n in df.columns if n != seqcol]\n",
    "features = pd.concat([df[seqcol], X, df[cols]],axis=1)\n",
    "print(features.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c661e-7111-4c12-a172-135c91ff7dec",
   "metadata": {
    "id": "8d0c661e-7111-4c12-a172-135c91ff7dec"
   },
   "source": [
    "**Step 3: Data Preprocessing and Normalization**\n",
    "<br>Once the features are extracted, the dataset is divided into training and test sets to ensure that the model generalizes well to unseen data. For small datasets, k-fold cross-validation can be used to get more reliable performance estimates (used in this study)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22b4cd7-6799-41f8-8478-c68ff8b4622e",
   "metadata": {
    "id": "b22b4cd7-6799-41f8-8478-c68ff8b4622e"
   },
   "outputs": [],
   "source": [
    "# Define train set for training\n",
    "train = features[features['split']=='train']\n",
    "X_train = train.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "y_train = train[ycol]\n",
    "\n",
    "# Define test set for evaluation\n",
    "test = features[features['split']=='test']\n",
    "X_test = test.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "y_test = test[ycol]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018442f-8fc3-4ae8-8868-95d465dcbb81",
   "metadata": {},
   "source": [
    "Define scoring metric to use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0b5d7ca-f5e9-481d-a054-67e40037418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this study, we will use spearman's correlation as the metric for evaluation\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "def spearmanr_metric(y_true, y_pred):\n",
    "    r = spearmanr(a=y_true, b=y_pred)\n",
    "    return r[0]\n",
    "    \n",
    "spearmanr_metric.__name__=\"Spearman\"\n",
    "\n",
    "def pearsonr_metric(y_true, y_pred):\n",
    "    r = pearsonr(y_true, y_pred)\n",
    "    return r[0] \n",
    "\n",
    "def set_scoring(metric):\n",
    "    if metric == 'mse':\n",
    "        return 'neg_root_mean_squared_error'\n",
    "    if metric == 'acc':\n",
    "        return 'accuracy'\n",
    "    elif metric == 'spr':\n",
    "        return make_scorer(spearmanr_metric)\n",
    "    elif metric == 'per':\n",
    "        return make_scorer(pearsonr_metric)\n",
    "    else:\n",
    "        print('wrong metric', metric)\n",
    "        exit()\n",
    "        \n",
    "def evaluate_test(y_test, y_pred, metric):\n",
    "    if metric == 'mse':\n",
    "        return root_mean_squared_error(y_test, y_pred)\n",
    "    if metric == 'acc':\n",
    "        return accuracy_score(y_test, y_pred)\n",
    "    elif metric == 'spr':\n",
    "        return spearmanr(y_test, y_pred)[0]\n",
    "    elif metric == 'per':\n",
    "        return pearsonr(y_test, y_pred)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b",
   "metadata": {
    "id": "8e1239ed-c1a3-49b1-8b47-1f8bf7d3333b"
   },
   "source": [
    "**Step 4: Model Selection: Use lazypredict to quickly compare different models on a regression task**\n",
    "<br> The first step in supervised learning is to compare and select a model for the dataset. \n",
    "\n",
    "In this step, the `LazyPredict` Python package is used to identify best model architecture that is suitable for the features. This Python package is designed to quickly try multiple machine learning models with minimal code (using default hyperparameters), making it easier for users to compare the performance of various algorithms without needing to manually implement each model. The package use its own data normalization steps, thus data pre-processing is not needed in this case.\n",
    "\n",
    "<br>Why lazypredict?\n",
    "* **Quick Prototyping:** To quickly get a sense of which machine learning models are working best for the dataset before diving into hyperparameter tuning and feature engineering.\n",
    "* **Model Comparison:** To get potential model candidates that might work best for a given problem/dataset, without having to implement them manually.\n",
    "* **Exploratory Data Analysis:** To quickly assess the performance of different models to gain insights into how well they fit the data.\n",
    "\n",
    "Several other AutoML tools are available: AutoGluon, PyCaret, and MLJAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6745241-5930-42e4-a98f-ed77cc5f3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.2\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
   "metadata": {
    "id": "5f703062-65b7-47e4-8f27-4e58c4a3e794",
    "outputId": "fe8d1b2c-74b8-4ed7-b62e-5fbb89c80923"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a3e50bdf81e4a6cabafd670b06920e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Model': 'AdaBoostRegressor', 'R-Squared': -0.18156759543190026, 'Adjusted R-Squared': -0.3510438977798196, 'RMSE': 1.0708033102675003, 'Time taken': 4.5498528480529785, 'Spearman': 0.3377907657529483}\n",
      "{'Model': 'BaggingRegressor', 'R-Squared': -0.11180725169241623, 'Adjusted R-Squared': -0.2712775881072853, 'RMSE': 1.0387120510972465, 'Time taken': 5.726940870285034, 'Spearman': 0.32418346920131774}\n",
      "{'Model': 'BayesianRidge', 'R-Squared': -0.013186531737191531, 'Adjusted R-Squared': -0.15851135923871595, 'RMSE': 0.9915739829720861, 'Time taken': 0.9075679779052734, 'Spearman': 0.5484985844661413}\n",
      "{'Model': 'DecisionTreeRegressor', 'R-Squared': -1.0984120308624115, 'Adjusted R-Squared': -1.3993944826221476, 'RMSE': 1.427006012306691, 'Time taken': 1.0001018047332764, 'Spearman': 0.11553341131049104}\n",
      "{'Model': 'DummyRegressor', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -1.4970998292969737, 'RMSE': 1.4557705433826131, 'Time taken': 0.03318977355957031, 'Spearman': nan}\n",
      "{'Model': 'ElasticNet', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -1.4970998292969737, 'RMSE': 1.4557705433826131, 'Time taken': 0.05785989761352539, 'Spearman': nan}\n",
      "{'Model': 'ElasticNetCV', 'R-Squared': -0.037250721071975246, 'Adjusted R-Squared': -0.1860271579805508, 'RMSE': 1.003280316487552, 'Time taken': 10.786766052246094, 'Spearman': 0.5458653778844472}\n",
      "{'Model': 'ExtraTreeRegressor', 'R-Squared': -1.078459479363913, 'Adjusted R-Squared': -1.3765800680669393, 'RMSE': 1.4202055326984608, 'Time taken': 0.2659800052642822, 'Spearman': 0.12637261105358086}\n",
      "{'Model': 'ExtraTreesRegressor', 'R-Squared': -0.05537229387561737, 'Adjusted R-Squared': -0.2067479702719408, 'RMSE': 1.012006410384532, 'Time taken': 9.783322811126709, 'Spearman': 0.4264211009681766}\n",
      "{'Model': 'GammaRegressor', 'R-Squared': -0.4671429096054851, 'Adjusted R-Squared': -0.6775802610504673, 'RMSE': 1.1932092973863733, 'Time taken': 0.19622302055358887, 'Spearman': 0.41736817380070534}\n",
      "{'Model': 'GaussianProcessRegressor', 'R-Squared': -4.606180718585686, 'Adjusted R-Squared': -5.410294492654454, 'RMSE': 2.332458717385729, 'Time taken': 2.2595810890197754, 'Spearman': 0.3615770846338571}\n",
      "{'Model': 'GradientBoostingRegressor', 'R-Squared': -0.03841255460625992, 'Adjusted R-Squared': -0.1873556372929488, 'RMSE': 1.0038420507253785, 'Time taken': 10.71332597732544, 'Spearman': 0.4320387411875728}\n",
      "{'Model': 'HistGradientBoostingRegressor', 'R-Squared': -0.16855818545489076, 'Adjusted R-Squared': -0.3361685034044941, 'RMSE': 1.0648920629263432, 'Time taken': 2.8163819313049316, 'Spearman': 0.4093214927797636}\n",
      "{'Model': 'HuberRegressor', 'R-Squared': -0.15032772415442897, 'Adjusted R-Squared': -0.31532318436483564, 'RMSE': 1.0565528177203718, 'Time taken': 0.5364673137664795, 'Spearman': 0.547214581785448}\n",
      "{'Model': 'KNeighborsRegressor', 'R-Squared': -1.0809369870986587, 'Adjusted R-Squared': -1.3794129332535539, 'RMSE': 1.4210517176552535, 'Time taken': 0.15885496139526367, 'Spearman': 0.29797214170847536}\n",
      "{'Model': 'KernelRidge', 'R-Squared': -8.477334998591495, 'Adjusted R-Squared': -9.83670173976105, 'RMSE': 3.032658687822202, 'Time taken': 0.5525031089782715, 'Spearman': 0.5429529788426314}\n",
      "{'Model': 'Lars', 'R-Squared': -0.8388653901285437, 'Adjusted R-Squared': -1.1026201749071785, 'RMSE': 1.335842906297903, 'Time taken': 0.8406600952148438, 'Spearman': 0.2352104853398294}\n",
      "{'Model': 'LarsCV', 'R-Squared': -3.663206073051006, 'Adjusted R-Squared': -4.332065751839138, 'RMSE': 2.1272705271622674, 'Time taken': 1.1712779998779297, 'Spearman': 0.07214326916025911}\n",
      "{'Model': 'Lasso', 'R-Squared': -1.1838611208002936, 'Adjusted R-Squared': -1.4970998292969737, 'RMSE': 1.4557705433826131, 'Time taken': 0.15275907516479492, 'Spearman': nan}\n",
      "{'Model': 'LassoCV', 'R-Squared': -0.046603462631044534, 'Adjusted R-Squared': -0.19672139541541656, 'RMSE': 1.007793383534768, 'Time taken': 9.91424298286438, 'Spearman': 0.5452626843402122}\n",
      "{'Model': 'LassoLars', 'R-Squared': -1.1827154888858185, 'Adjusted R-Squared': -1.4957898754584145, 'RMSE': 1.455388651887544, 'Time taken': 0.5966691970825195, 'Spearman': nan}\n",
      "{'Model': 'LassoLarsCV', 'R-Squared': -0.03946710899827566, 'Adjusted R-Squared': -0.1885614500468853, 'RMSE': 1.004351644628942, 'Time taken': 1.8470649719238281, 'Spearman': 0.5461132370464874}\n",
      "{'Model': 'LassoLarsIC', 'R-Squared': -0.879307775027756, 'Adjusted R-Squared': -1.1488633501101773, 'RMSE': 1.350452690335512, 'Time taken': 1.3216772079467773, 'Spearman': 0.2352104853398294}\n",
      "{'Model': 'LinearRegression', 'R-Squared': -0.16817174768750953, 'Adjusted R-Squared': -0.3357266375395951, 'RMSE': 1.0647159704802753, 'Time taken': 0.38907575607299805, 'Spearman': 0.5363125580589237}\n",
      "{'Model': 'LinearSVR', 'R-Squared': -0.2603928597300009, 'Adjusted R-Squared': -0.4411753407311665, 'RMSE': 1.1059446351186084, 'Time taken': 1.261850118637085, 'Spearman': 0.5192882666271121}\n",
      "{'Model': 'MLPRegressor', 'R-Squared': -0.8405518625291144, 'Adjusted R-Squared': -1.1045485438421205, 'RMSE': 1.3364553344565433, 'Time taken': 6.922555208206177, 'Spearman': 0.43496192682430673}\n",
      "{'Model': 'NuSVR', 'R-Squared': -0.3215179084827986, 'Adjusted R-Squared': -0.5110677653696187, 'RMSE': 1.1324445487322505, 'Time taken': 1.404728889465332, 'Spearman': 0.494858327913546}\n",
      "{'Model': 'OrthogonalMatchingPursuit', 'R-Squared': -0.8506940626352439, 'Adjusted R-Squared': -1.116145474577547, 'RMSE': 1.3401324858929144, 'Time taken': 0.49095702171325684, 'Spearman': 0.2352104853398294}\n",
      "{'Model': 'OrthogonalMatchingPursuitCV', 'R-Squared': -0.36870557841822515, 'Adjusted R-Squared': -0.5650237250313277, 'RMSE': 1.152485414467614, 'Time taken': 0.14242887496948242, 'Spearman': 0.44613423990598045}\n",
      "{'Model': 'PassiveAggressiveRegressor', 'R-Squared': -1.0177527362049763, 'Adjusted R-Squared': -1.307165948031777, 'RMSE': 1.3993114621998226, 'Time taken': 0.1033930778503418, 'Spearman': 0.39243849963652955}\n",
      "{'Model': 'PoissonRegressor', 'R-Squared': -0.3202910257142788, 'Adjusted R-Squared': -0.509664906587685, 'RMSE': 1.1319187521030365, 'Time taken': 0.48816800117492676, 'Spearman': 0.4694399215282209}\n",
      "{'Model': 'QuantileRegressor', 'R-Squared': -2.160015827413052, 'Adjusted R-Squared': -2.613267761421199, 'RMSE': 1.7511568530421915, 'Time taken': 0.4715540409088135, 'Spearman': nan}\n",
      "{'Model': 'RANSACRegressor', 'R-Squared': -29.862946457656733, 'Adjusted R-Squared': -34.289724972425965, 'RMSE': 5.47266811867519, 'Time taken': 4.308682203292847, 'Spearman': 0.13518818135177338}\n",
      "{'Model': 'RandomForestRegressor', 'R-Squared': -0.05018378017955505, 'Adjusted R-Squared': -0.20081525021875613, 'RMSE': 1.009515688065003, 'Time taken': 56.83242416381836, 'Spearman': 0.40935221336314787}\n",
      "{'Model': 'Ridge', 'R-Squared': -0.11900287544274679, 'Adjusted R-Squared': -0.27950530491010617, 'RMSE': 1.0420679056450046, 'Time taken': 0.34018826484680176, 'Spearman': 0.542586522017243}\n",
      "{'Model': 'RidgeCV', 'R-Squared': -0.024435597930791086, 'Adjusted R-Squared': -0.17137391766985566, 'RMSE': 0.9970633431049741, 'Time taken': 0.4096670150756836, 'Spearman': 0.5505488928459596}\n",
      "{'Model': 'SGDRegressor', 'R-Squared': -0.13933351662257776, 'Adjusted R-Squared': -0.30275203984948273, 'RMSE': 1.0514917169170108, 'Time taken': 0.20949816703796387, 'Spearman': 0.5229585125112917}\n",
      "{'Model': 'SVR', 'R-Squared': -0.30627637970617516, 'Adjusted R-Squared': -0.4936400917214041, 'RMSE': 1.1258951701410058, 'Time taken': 1.289262056350708, 'Spearman': 0.49469576354772554}\n",
      "{'Model': 'TransformedTargetRegressor', 'R-Squared': -0.16817174768750953, 'Adjusted R-Squared': -0.3357266375395951, 'RMSE': 1.0647159704802753, 'Time taken': 0.10967707633972168, 'Spearman': 0.5363125580589237}\n",
      "{'Model': 'TweedieRegressor', 'R-Squared': -0.47685557042961557, 'Adjusted R-Squared': -0.6886860422079557, 'RMSE': 1.1971523760791536, 'Time taken': 0.28743600845336914, 'Spearman': 0.4234034666426385}\n",
      "{'Model': 'XGBRegressor', 'R-Squared': -0.1588443324731159, 'Adjusted R-Squared': -0.3250613590940916, 'RMSE': 1.060456771681096, 'Time taken': 1.2243778705596924, 'Spearman': 0.3632732598351747}\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 80037\n",
      "[LightGBM] [Info] Number of data points in the train set: 2097, number of used features: 320\n",
      "[LightGBM] [Info] Start training from score 3.186066\n",
      "{'Model': 'LGBMRegressor', 'R-Squared': -0.1894730541136711, 'Adjusted R-Squared': -0.36008326357865306, 'RMSE': 1.0743795250836234, 'Time taken': 0.623737096786499, 'Spearman': 0.39655697683100505}\n",
      "                               Spearman  Time Taken\n",
      "Model                                              \n",
      "RidgeCV                            0.55        0.41\n",
      "BayesianRidge                      0.55        0.91\n",
      "HuberRegressor                     0.55        0.54\n",
      "LassoLarsCV                        0.55        1.85\n",
      "ElasticNetCV                       0.55       10.79\n",
      "LassoCV                            0.55        9.91\n",
      "KernelRidge                        0.54        0.55\n",
      "Ridge                              0.54        0.34\n",
      "LinearRegression                   0.54        0.39\n",
      "TransformedTargetRegressor         0.54        0.11\n",
      "SGDRegressor                       0.52        0.21\n",
      "LinearSVR                          0.52        1.26\n",
      "NuSVR                              0.49        1.40\n",
      "SVR                                0.49        1.29\n",
      "PoissonRegressor                   0.47        0.49\n",
      "OrthogonalMatchingPursuitCV        0.45        0.14\n",
      "MLPRegressor                       0.43        6.92\n",
      "GradientBoostingRegressor          0.43       10.71\n",
      "ExtraTreesRegressor                0.43        9.78\n",
      "TweedieRegressor                   0.42        0.29\n",
      "GammaRegressor                     0.42        0.19\n",
      "RandomForestRegressor              0.41       56.83\n",
      "HistGradientBoostingRegressor      0.41        2.82\n",
      "LGBMRegressor                      0.40        0.62\n",
      "PassiveAggressiveRegressor         0.39        0.10\n",
      "XGBRegressor                       0.36        1.22\n",
      "GaussianProcessRegressor           0.36        2.26\n",
      "AdaBoostRegressor                  0.34        4.55\n",
      "BaggingRegressor                   0.32        5.73\n",
      "KNeighborsRegressor                0.30        0.16\n",
      "Lars                               0.24        0.84\n",
      "OrthogonalMatchingPursuit          0.24        0.49\n",
      "LassoLarsIC                        0.24        1.32\n",
      "RANSACRegressor                    0.14        4.31\n",
      "ExtraTreeRegressor                 0.13        0.27\n",
      "DecisionTreeRegressor              0.12        1.00\n",
      "LarsCV                             0.07        1.17\n",
      "LassoLars                           NaN        0.60\n",
      "Lasso                               NaN        0.15\n",
      "ElasticNet                          NaN        0.06\n",
      "DummyRegressor                      NaN        0.03\n",
      "QuantileRegressor                   NaN        0.47\n"
     ]
    }
   ],
   "source": [
    "# Incompatibilities between lazypredict and sklearn might cause errors\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import make_scorer\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "\n",
    "# Create a LazyRegressor object\n",
    "rgs = LazyRegressor(verbose=1, ignore_warnings=False, custom_metric=spearmanr_metric)\n",
    "\n",
    "# Fit the models and get a comparison\n",
    "models, predictions = rgs.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Sort results and show only the selected metric\n",
    "sorted_models = models.sort_values(by='Spearman', ascending=False)\n",
    "\n",
    "# Show model comparison results\n",
    "print(sorted_models[['Spearman','Time Taken']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecd25c35-0808-431f-b8c3-f933e4edeb5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Adjusted R-Squared  R-Squared  RMSE  Time Taken  Spearman\n",
      "Model                                                                    \n",
      "RidgeCV                      -0.17      -0.02  1.00        0.46      0.55\n",
      "BayesianRidge                -0.16      -0.01  0.99        0.63      0.55\n",
      "HuberRegressor               -0.32      -0.15  1.06        0.40      0.55\n",
      "LassoLarsCV                  -0.19      -0.04  1.00        0.85      0.55\n",
      "ElasticNetCV                 -0.19      -0.04  1.00        8.36      0.55\n"
     ]
    }
   ],
   "source": [
    "# Print top 5 models for evaluation\n",
    "print(sorted_models.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85958dc2-06cd-48a3-a688-af839b194209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try autogluon (much better compatibility with latest sklearn)  [NOT TESTED DUE TO COMPATIBILITY ISSUE]  \n",
    "# It might take some time, but it uses lesser models than lazypredict and pycaret\n",
    "from autogluon.tabular import TabularPredictor\n",
    "train = pd.concat([X_train,y_train], axis=1)\n",
    "test = pd.concat([X_test,y_test], axis=1)\n",
    "\n",
    "predictor = TabularPredictor(label=ycol, verbosity=1, eval_metric=\"spearmanr\", problem_type=\"regression\").fit(train)\n",
    "y_pred = predictor.predict(test.drop(columns=[ycol]))\n",
    "predictor.evaluate(test)\n",
    "predictor.leaderboard(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4044b-2337-4c99-abef-533761fed16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try pycaret  [NOT TESTED DUE TO COMPATIBILITY ISSUE] \n",
    "from pycaret.regression import *\n",
    "s = setup(train, target = ycol, session_id = 123, verbose=1)\n",
    "\n",
    "# model training and selection\n",
    "best = compare_models()\n",
    "\n",
    "# evaluate trained model\n",
    "evaluate_model(best)\n",
    "\n",
    "# predict on hold-out/test set\n",
    "predict_model(best, data=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08413e4-f251-400f-91c9-90dc43af76c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try MLJAR  [NOT TESTED DUE TO COMPATIBILITY ISSUE] \n",
    "from supervised.automl import AutoML\n",
    "automl = AutoML()\n",
    "automl.fit(X_train, y_train)\n",
    "\n",
    "predictions = automl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615dd001-aa62-491a-899f-07d0b1bb2b23",
   "metadata": {
    "id": "615dd001-aa62-491a-899f-07d0b1bb2b23"
   },
   "source": [
    "**Step 5: Model Construction and Hyperparameter Tuning: Use Nested K-fold Cross Validation to Estimate Model Performance**\n",
    "<br>Data scaling and normalization are necessary to ensure that all features contribute equally to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2f60147d-5701-4d5e-b495-cc5278227e24",
   "metadata": {
    "id": "2f60147d-5701-4d5e-b495-cc5278227e24"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the train set \n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test set (not fit to ensure the set remains unseen)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3068d0ac-2afe-487f-b157-a34ac40595d9",
   "metadata": {
    "id": "3068d0ac-2afe-487f-b157-a34ac40595d9"
   },
   "source": [
    "Hyperparameter optimization (HPO) is the process of finding the best set of hyperparameters for a machine learning model to maximize its performance on a validation (test) set.\n",
    "\n",
    "**(A) Nested cross-validation for small dataset**\n",
    "When training data are limited, double cross-validation (nested cross-validation) is the standard approach for evaluating and comparing machine learning models with tuned hyperparameters. Nested cross-validation is particularly important when a fixed, untouched test set is not available. Without nested cross-validation, the same data may be used both for hyperparameter selection and performance evaluation, leading to optimistically biased estimates and increased risk of overfitting.\n",
    "\n",
    "* **Inner Cross-Validation (Hyperparameter Tuning):**\n",
    "  - Split the data into N_SPLITS folds\n",
    "  - Evaluate the model's performance using each fold as a validation set while training on the remaining folds\n",
    "  - Estimate the best set of hyperparameters for the model\n",
    "* **Outer Cross-Validation (Model Evaluation):**\n",
    "  - Assess the model's generalization performance\n",
    "  - Ensures that the model is tested on different, unseen data splits\n",
    "\n",
    "**(B) Standard Inner Cross-validation**\n",
    "<br> K-fold cross-validation can be used to estimate model performance by producing multiple performance estimates across different data splits, which is particularly suitable for medium to large datasets.\n",
    "\n",
    "*Hyperparameter tuning*\n",
    "<br>Here, Support Vector Regressor (SVR) will be used for further training and evaluation. To find the optimal hyperparameters for the model, several HPO algorithms will be tested.\n",
    "\n",
    "*Why use K-fold instead of a single train-test split?*\n",
    "<br>A single train-test split can produce a noisy or biased estimate of model performance, particularly if the test set is small or not representative of the population. K-fold cross-validation mitigates this issue by evaluating the model across multiple folds of the data, providing a more robust and stable estimate of generalization performance.\n",
    "\n",
    "*Example on how to use K-fold with GridSearchCV*\n",
    "```\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "gsmodel = GridSearchCV(estimator=model, param_grid=param_grid, cv=cv, scoring=scoring, verbose=1)\n",
    "gsmodel.fit(X, y)\n",
    "print(gsmodel.best_params_)\n",
    "print(gsmodel.best_score_)\n",
    "```\n",
    "\n",
    "To allow proper comparison on different approaches that use train/test datasets (such as protein language model finetuning later on), hyperparameter tuning and model training are performed on the train set and evaluated on the test set. In the original study, several random seeds were used to obtain average performance scores of the model.\n",
    "\n",
    "**Hyperparameter Tuning:**\n",
    "<br> Hyperparameter optimization can be performed using a range of search strategies that differ in efficiency and scalability. GridSearchCV performs an exhaustive evaluation of all parameter combinations in a predefined grid and is straightforward to implement but becomes computationally prohibitive as the search space grows. RandomizedSearchCV improves efficiency by sampling hyperparameter configurations at random, often achieving comparable performance with substantially fewer evaluations. More advanced methods, such as Optuna (including OptunaSearchCV), Hyperopt, and scikit-optimize (skopt), employ Bayesian or adaptive optimization strategies that leverage information from previous trials to guide subsequent searches toward promising regions of the parameter space. These methods are generally more sample-efficient and better suited for large or continuous search spaces, though they introduce additional complexity and dependencies. Overall, grid and random search provide simple and reproducible baselines, while Bayesian optimization frameworks offer improved efficiency for computationally expensive models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ae447d8-1461-4dad-ba3d-da7de3f66cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for HPO algorithms\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "import optuna\n",
    "from optuna.distributions import FloatDistribution, IntDistribution, CategoricalDistribution\n",
    "from optuna_integration.sklearn import OptunaSearchCV\n",
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK \n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real\n",
    "import numpy as np\n",
    "\n",
    "# set verbose=1 to show errors, warning and std output (e.g. trial log)\n",
    "# Hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def default(model, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    val = evaluate_test(y_test, y_pred, metric)\n",
    "    return val, model\n",
    "    \n",
    "def train(model, params, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    val = evaluate_test(y_test, y_pred, metric)\n",
    "    return val, model\n",
    "     \n",
    "def grid_search(model, param_grid, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    reg = model\n",
    "    optim = GridSearchCV(estimator=model, param_grid=param_grid, \n",
    "                         cv=5, scoring=scoring, verbose=0)\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def random_search(model, param_grid, seed, X_train, X_test, y_train, y_test, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    reg = model\n",
    "    optim = RandomizedSearchCV(estimator=reg, param_distributions=param_grid, \n",
    "                               cv=5, scoring=scoring, verbose=0)\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def optuna_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = CategoricalDistribution(values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                param_distributions[name] = IntDistribution(low=values[0], high=values[1], log=True)\n",
    "            else:\n",
    "                param_distributions[name] = FloatDistribution(low=values[0],high=values[1],log=True)\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = CategoricalDistribution(values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "            \n",
    "    # set up training \n",
    "    reg = model\n",
    "    optim = optuna.integration.OptunaSearchCV(\n",
    "        reg, param_distributions, n_trials=n_trials, random_state=seed, scoring=scoring, verbose=0\n",
    "    )\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "\n",
    "def hyperopt_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = hp.choice(name, values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                #param_distributions[name] = hp.quniform(name, values[0], values[1], q=1)\n",
    "                param_distributions[name] = hp.choice(name, np.arange(values[0], values[1], dtype=int))\n",
    "            else:\n",
    "                param_distributions[name] = hp.loguniform(\n",
    "                    name,\n",
    "                    np.log(values[0]),\n",
    "                    np.log(values[1])\n",
    "                )\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = hp.choice(name, values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "    # set up training\n",
    "    reg = model\n",
    "    def objective(params):\n",
    "        # Create model with sampled params\n",
    "        reg.set_params(**params)\n",
    "        # Cross-validation score (Hyperopt minimizes, so we negate scoring)\n",
    "        cv_scores = cross_val_score( model, X_train, y_train, scoring=scoring, cv=5, verbose=0)\n",
    "        loss = -np.mean(cv_scores)\n",
    "        return { 'loss': loss, 'status': STATUS_OK, 'params': params }\n",
    "    trials = Trials()\n",
    "    # Run TPE optimization\n",
    "    optim = fmin(fn=objective, space=param_distributions, algo=tpe.suggest, \n",
    "                 max_evals=n_trials, rstate=np.random.default_rng(seed), trials=trials, verbose=0 )\n",
    "    score, est = train(model, optim, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim)\n",
    "\n",
    "def skopt_search(model, param_grid, seed, X_train, X_test, y_train, y_test, n_trials, metric):\n",
    "    scoring=set_scoring(metric)\n",
    "    # set up param_distributions for optuna\n",
    "    param_distributions = {}\n",
    "    for name, values in param_grid.items():\n",
    "        if len(values) == 2:\n",
    "            if isinstance(values[0], str):\n",
    "                param_distributions[name] = Categorical(values)\n",
    "            elif all(isinstance(v, int) for v in values):\n",
    "                param_distributions[name] = Integer(\n",
    "                    values[0],\n",
    "                    values[1]\n",
    "                )\n",
    "            else:\n",
    "                param_distributions[name] = Real(\n",
    "                    values[0],\n",
    "                    values[1],\n",
    "                    prior='log-uniform'\n",
    "                )\n",
    "        elif len(values) > 2:\n",
    "            param_distributions[name] = Categorical(values)\n",
    "        elif len(values) == 1:\n",
    "            print(\"Error. Param ranges or list of values was not specified.\")\n",
    "            exit()\n",
    "        else:\n",
    "            print(\"Error. Something wrong with the param grid.\")\n",
    "            exit()\n",
    "     # set up training\n",
    "    reg = model\n",
    "    optim = BayesSearchCV(\n",
    "        estimator=reg,\n",
    "        search_spaces=param_distributions,\n",
    "        n_iter=n_trials,\n",
    "        random_state=seed,\n",
    "        scoring=scoring,\n",
    "        cv=5,                 # same as OptunaSearchCV default\n",
    "        verbose=0,\n",
    "        optimizer_kwargs={\"base_estimator\": \"GP\"}  # Gaussian Process BO\n",
    "    )\n",
    "    optim.fit(X_train, y_train)\n",
    "    score, est = train(model, optim.best_params_, seed, X_train, X_test, y_train, y_test, metric)\n",
    "    return (score, est, optim.best_params_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2e34bc37-7a46-4074-b8ee-3784ec6e4f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default: 0.4947\n",
      "gridsearchCV: 0.4827\n",
      "randomsearchCV: 0.4827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-12-24 20:07:40,508] Trial 3 failed with parameters: {'gamma': 0.0003139312453909013, 'C': 0.16358767917148717, 'epsilon': 1.9886440867814372} because of the following error: The value nan is not acceptable.\n",
      "[W 2025-12-24 20:07:40,508] Trial 3 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optuna: 0.5365\n",
      "hyperopt: 0.5459\n",
      "skopt: 0.4990\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import make_scorer\n",
    "#from sklearnex import patch_sklearn\n",
    "#patch_sklearn()\n",
    "\n",
    "# Define model\n",
    "model = SVR()\n",
    "metric = 'spr'\n",
    "n_trials=5\n",
    "param_grid = { 'gamma': [1e-5, 0.5], 'C':[0.01, 30.0], 'epsilon':[0.001, 2.0] }\n",
    "\n",
    "default, d_estimator = default(model, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'default: {default:.4f}')\n",
    "\n",
    "gridsearch, g_estimator, g_params = grid_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'gridsearchCV: {gridsearch:.4f}')\n",
    "\n",
    "randomsearch, r_estimator, r_params = random_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, metric)\n",
    "print(f'randomsearchCV: {randomsearch:.4f}')\n",
    "\n",
    "optuna, o_estimator, o_params = optuna_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'optuna: {optuna:.4f}')\n",
    "\n",
    "hyperopt, h_estimator, h_params = hyperopt_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'hyperopt: {hyperopt:.4f}')\n",
    "\n",
    "skopt, s_estimator, s_params = skopt_search(model, param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'skopt: {skopt:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0fa390d-e90e-4b90-bbbd-6d87a5dc9e83",
   "metadata": {
    "id": "c0fa390d-e90e-4b90-bbbd-6d87a5dc9e83",
    "outputId": "3a0be294-c5f9-42da-dda6-e40e158091d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR: 0.3080, {'C': 9, 'epsilon': 0.5192261094259663, 'gamma': 0.12315040349850609}\n",
      "Lasso: 0.5432, {'alpha': 0.0012668423699634733}\n",
      "ETR: 0.3844, {'n_estimators': 33}\n"
     ]
    }
   ],
   "source": [
    "# Try three different model architecture\n",
    "# Note that the param_grid requires a range for float or integers (lower_value, upper_value)\n",
    "from sklearn.svm import SVR\n",
    "SVR_param_grid = { 'gamma': [0.1, 0.2], 'C':[20, 30], 'epsilon':[0.1, 1.5] }\n",
    "SVR_hyperopt, SVR_estimator, SVR_params = hyperopt_search(SVR(), SVR_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'SVR: {SVR_hyperopt:.4f}, {SVR_params}')\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "Lasso_param_grid = { 'alpha': [1e-3, 5e-3] }\n",
    "Lasso_hyperopt, Lasso_estimator, Lasso_params = hyperopt_search(Lasso(), Lasso_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'Lasso: {Lasso_hyperopt:.4f}, {Lasso_params}')\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "ETR_param_grid = { 'n_estimators': [10, 50] }\n",
    "ETR_hyperopt, ETR_estimator, ETR_params = hyperopt_search(ExtraTreesRegressor(random_state=42), ETR_param_grid, seed, X_train_scaled, X_test_scaled, y_train, y_test, n_trials, metric)\n",
    "print(f'ETR: {ETR_hyperopt:.4f}, {ETR_params}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29042c9-688b-454e-8db9-885ee56f2a06",
   "metadata": {
    "id": "a29042c9-688b-454e-8db9-885ee56f2a06"
   },
   "source": [
    "Here, train set was used for hyperparameter tuning to obtain unbiased performance estimate using k-fold cross-validation (CV). Instead of using the best estimator output from HPO, the model is re-built with best parameters from HP tuning, re-trained with training set and evaluated on unseen test set. By using the train set for model construction and hyperparameter tuning, the test set remains truly unseen throughout the entire model selection process, ensuring an unbiased evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5",
   "metadata": {
    "id": "17191ef5-8e89-45ef-88cf-cac7ee36d4a5"
   },
   "source": [
    "**Step 8: Model Prediction**\n",
    "<br>After evaluating several models based on their performance metrics, the best model can be saved for prediction on unseen or real data. For example, the model can be used to predict the fluorescent intensity of variants, given the amino acid features as the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c28777e3-fe60-4e65-893c-0f1b733f413d",
   "metadata": {
    "id": "c28777e3-fe60-4e65-893c-0f1b733f413d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id                                             mutant\n",
      "0  S1A  AKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "1  S1C  CKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "2  S1D  DKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "3  S1E  EKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "4  S1F  FKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFI...\n",
      "4503\n"
     ]
    }
   ],
   "source": [
    "# Define a function to generate all single-residue mutants\n",
    "def single_residue_mutants(sequence, amino_acids=None):\n",
    "    if amino_acids is None:\n",
    "        amino_acids = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "    mutants = []\n",
    "    for i, wt in enumerate(sequence):\n",
    "        for aa in amino_acids:\n",
    "            if aa != wt:\n",
    "                mutant = sequence[:i] + aa + sequence[i+1:]\n",
    "                label = f\"{wt}{i+1}{aa}\"\n",
    "                mutants.append([label]+[mutant])\n",
    "    mutants = pd.DataFrame(mutants)\n",
    "    mutants.columns = ['id','mutant']\n",
    "    return mutants\n",
    "\n",
    "# Generate the list of mutants for prediction\n",
    "sequence = \"SKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTLSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK\"\n",
    "mutants = single_residue_mutants(sequence)\n",
    "print(mutants.head())\n",
    "print(len(mutants))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac",
   "metadata": {
    "id": "68326658-e8f7-46c0-8811-ddbe32f2d0ac"
   },
   "source": [
    "**Perform prediction on the prediction list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
   "metadata": {
    "id": "ea6aab92-84a0-43ee-ad34-344a50927511",
    "outputId": "373a630c-9657-4555-d2e2-375addd84e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.16</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>...</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  320 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1    2    3    4     5    6     7     8     9    ...  310  311   312  \\\n",
       "0 -0.05 0.03 0.20 0.16 0.24 -0.03 0.00 -0.09 -0.05 -0.18  ... 0.12 0.03 -0.13   \n",
       "1 -0.06 0.03 0.20 0.16 0.22 -0.04 0.00 -0.10 -0.05 -0.18  ... 0.12 0.03 -0.11   \n",
       "2 -0.05 0.03 0.20 0.16 0.24 -0.04 0.01 -0.09 -0.05 -0.19  ... 0.12 0.03 -0.13   \n",
       "3 -0.05 0.03 0.20 0.17 0.24 -0.04 0.02 -0.09 -0.05 -0.19  ... 0.13 0.03 -0.13   \n",
       "4 -0.06 0.03 0.20 0.15 0.23 -0.04 0.01 -0.09 -0.05 -0.18  ... 0.13 0.03 -0.12   \n",
       "\n",
       "   313  314   315   316  317  318  319  \n",
       "0 0.09 0.14 -0.19 -0.14 0.12 0.02 0.03  \n",
       "1 0.08 0.13 -0.17 -0.16 0.10 0.04 0.03  \n",
       "2 0.09 0.14 -0.18 -0.14 0.11 0.02 0.02  \n",
       "3 0.09 0.14 -0.17 -0.14 0.10 0.02 0.03  \n",
       "4 0.09 0.14 -0.17 -0.14 0.11 0.02 0.03  \n",
       "\n",
       "[5 rows x 320 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute amino acid features for 20 sequences in the dataset\n",
    "print(len(sequence))\n",
    "\n",
    "#X_df = mutants.head(100)\n",
    "X_pred = compute_embeddings(mutants, 'mutant', checkpoint, 'mean')\n",
    "X_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c31c5a-d108-4fea-86ac-1cf0462bca29",
   "metadata": {},
   "source": [
    "**Make prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "656074e3-9912-4621-9c38-477cb54cfe71",
   "metadata": {
    "id": "656074e3-9912-4621-9c38-477cb54cfe71",
    "outputId": "c3f25706-92fd-4826-8f50-bbf41283c269"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pred_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2266</th>\n",
       "      <td>N120G</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>L14V</td>\n",
       "      <td>4.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2353</th>\n",
       "      <td>L124V</td>\n",
       "      <td>4.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>D116K</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3360</th>\n",
       "      <td>L177V</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  pred_score\n",
       "2266  N120G        4.46\n",
       "263    L14V        4.39\n",
       "2353  L124V        4.32\n",
       "2192  D116K        4.28\n",
       "3360  L177V        4.28"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "X_pred_scaled = scaler.transform(X_pred)\n",
    "\n",
    "y_pred = h_estimator.predict(X_pred_scaled)\n",
    "results = pd.concat([mutants[['id']], pd.DataFrame(y_pred, columns=['pred_score'])], axis=1)\n",
    "results = results.sort_values(by='pred_score', ascending=False)\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f93991-2960-4afc-8c40-da0335efbcaa",
   "metadata": {},
   "source": [
    "**Evaluation of different pooling strategies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d86e285-cce5-49ca-9989-66825e948f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.5459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max: 0.5598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum: 0.5457\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def pool_workflow(pool, df, param_grid, seed, n_trials, metric):\n",
    "    X = compute_embeddings(df, 'sequence', checkpoint, pool)\n",
    "    cols = [n for n in df.columns if n != seqcol]\n",
    "    embed = pd.concat([df[seqcol], X, df[cols]],axis=1)\n",
    "\n",
    "    train_df = embed[embed['split']=='train']\n",
    "    Xtrain = train_df.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "    ytrain = train_df[ycol]\n",
    "    \n",
    "    # Define test set for evaluation\n",
    "    test_df = embed[embed['split']=='test']\n",
    "    Xtest = test_df.loc[:, seqcol:ycol].iloc[:, 1:-1]\n",
    "    ytest = test_df[ycol]\n",
    "\n",
    "    # Feature scaling\n",
    "    Scaler = StandardScaler() \n",
    "    XtrainScaled = Scaler.fit_transform(Xtrain)\n",
    "    XtestScaled = Scaler.transform(Xtest)\n",
    "\n",
    "    # Optimize hyperparameters\n",
    "    score, estimator, params = hyperopt_search(SVR(), param_grid, seed, XtrainScaled, XtestScaled, ytrain, ytest, n_trials, metric)\n",
    "    return score\n",
    "\n",
    "for pool in ['mean', 'max', 'sum']:\n",
    "    score = pool_workflow(pool, df, param_grid, 42, 5, 'spr')\n",
    "    print(f'{pool}: {score:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EmFVfXhWXJey",
   "metadata": {
    "id": "EmFVfXhWXJey"
   },
   "source": [
    "**References:** <br>\n",
    "* [Fluorescence TAPE benchmark dataset](https://github.com/songlab-cal/tape)\n",
    "* [ESM-2 models](https://github.com/facebookresearch/esm)\n",
    "* [Protein Language Model](https://huggingface.co/models?other=protein+language+model)\n",
    "* [scikit-learn.org](https://scikit-learn.org/1.5/auto_examples/model_selection/plot_nested_cross_validation_iris.html)\n",
    "* [LazyPredict](https://lazypredict.readthedocs.io/en/latest/)\n",
    "* [PyCaret](https://github.com/pycaret/pycaret)\n",
    "* [AutoGluon](https://auto.gluon.ai/stable/tutorials/tabular/tabular-quick-start.html)\n",
    "* [MLJAR](https://supervised.mljar.com)\n",
    "* [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "* [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html)\n",
    "* [Optuna](https://optuna.readthedocs.io/en/v2.0.0/reference/generated/optuna.integration.OptunaSearchCV.html)\n",
    "* [Hyperopt](https://github.com/hyperopt/hyperopt)\n",
    "* [Skopt](https://scikit-optimize.github.io/stable/)\n",
    "* [Model selection: Nested Cross-Validation for Machine Learning with Python](https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/)\n",
    "* [Model selection: Training-validation-test split and cross-validation done right](https://machinelearningmastery.com/training-validation-test-split-and-cross-validation-done-right/)\n",
    "* [Model construction: How to Train a Final Machine Learning Model](https://machinelearningmastery.com/train-final-machine-learning-model/)\n",
    "* [Model training: Embrace Randomness in Machine Learning](https://machinelearningmastery.com/randomness-in-machine-learning/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f91cb3-4af7-4354-ad2f-471e63d8a979",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
