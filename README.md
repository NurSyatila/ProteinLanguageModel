# Protein Language Model
Protein language models, such as ESM-2 is a deep learning model trained on large datasets of protein sequences, learning patterns, relationships, and general features in those sequences. Like natural language processing (NLP) models such as GPT, these models are trained to predict the next amino acid in a sequence, or to understand the context of protein sequences in a way similar to how language models understand words in a sentence.

Protein language models (PLMs) can be extremely powerful tools for binding affinity improvement, particularly because they capture rich, high-dimensional representations of protein sequences. These representations can be leveraged in a variety of ways to predict and improve protein-ligand binding affinity or other properties relevant to molecular interactions. There are several approaches that can be used to achieve this, spanning embeddings, supervised learning, unsupervised learning, and even hybrid strategies. Here are some approaches that will be highlighted in this repository:

1. Supervised Learning on Amino Acid Descriptors (SL-AAFeat): Amino acid descriptors are hand-crafted numerical features that encode known physicochemical, biochemical, or structural properties of amino acids. Each amino acid is mapped to a fixed-length vector (typically 3–10 dimensions), depending on the type of the descriptors. Supervised models trained on handcrafted amino-acid descriptors can serve as a baseline to assess whether improvements arise from learned sequence representations rather than known physicochemical properties.
   
2. Supervised Learning on PLM Embeddings (SL-Embed): Protein language models can generate protein embeddings (dense vector representations) for each protein sequence from the last hidden state of the models, which can be used as input features for downstream tasks like binding affinity prediction. Once a protein sequence is passed through a language model like ESM-2, the output is typically a high-dimensional embedding that encodes various structural, functional, and evolutionary features of the protein. These embeddings can then be used as input features for regression models (e.g., neural networks, support vector machines, random forests) to predict binding affinity.

3. Supervised fine-tuning (FT-SV-Full & FT-SV-LoRA): Supervised learning involves training a model on labeled data, where the goal is to predict a continuous output (e.g., binding affinity) based on input features (e.g., protein sequences and ligand structures). Initially, the model is pre-trained on a large dataset of protein sequences to capture general sequence-function relationships. Then, the pre-trained model can be fine-tuned for specific tasks such as binding affinity regression, using labeled examples where the input is the protein-ligand pair and the output is a continuous binding affinity value. Low-Rank Adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) technique designed to adapt large pretrained models while updating only a small subset of parameters. Instead of modifying the original model weights, LoRA introduces two trainable low-rank matrices into selected linear layers. Full-model fine-tuning updates all layers, allowing maximal flexibility and task-specific adaptation. However, this comes at the cost of increased memory usage, longer training times, and a higher risk of catastrophic forgetting—especially when fine-tuning on small or noisy datasets. In contrast, LoRA restricts learning to additive low-rank updates, significantly reducing the number of trainable parameters while preserving the expressive capacity of the pretrained model.

4. Self-supervised fine-tuning (FT-MLM): Fine-tuning a Protein Language Model (PLM) on a masked language model (MLM) task is considered self-supervised, because it doesn’t require manually labeled data. Instead, the model learns from the input sequence by masking parts of the sequence (amino acids, in the case of protein sequences) and learning to predict the masked parts. The model is essentially learning the underlying structure and relationships in the data without needing external supervision (labels) during the learning process.

5. Zero-shot prediction (ZSHOT): Zero-shot prediction using protein language models involves generalizing learned knowledge to new tasks without task-specific fine-tuning, which allow the model to make predictions or perform tasks without having been specifically trained on specific tasks. It leverages the general knowledge the model has acquired during its pre-training phase, where it learns from a vast corpus of protein sequences and their relationships (sequence-function, sequence-structure, etc.) without explicit task-specific labels. 
 
